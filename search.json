[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Ai-Log",
    "section": "",
    "text": "12 - Accelerated SGD\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n11 - Initialization\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n10 - Activations\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n09 - Learner\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n08 - Autoencoders and how manual coding doesn’t work\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n07 - Convolutions\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n06 - Callbacks\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n05 - Datasets\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n04 - Minibatch Training\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n03 - The forward and backward passes\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n02 - Mean Shift\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n01 - Matrix multiplication from foundations\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\nTiny bug world with neuro evolution (2018)\n\n\n\n\n\n\n\nneuro evolution\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2022\n\n\nNiyazi Kemer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/course22p2/11_initializing.html",
    "href": "posts/course22p2/11_initializing.html",
    "title": "11 - Initialization",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nInitialization, Layer normalization, Batch Normalization, cleaning memory"
  },
  {
    "objectID": "posts/course22p2/11_initializing.html#glorotxavier-init",
    "href": "posts/course22p2/11_initializing.html#glorotxavier-init",
    "title": "11 - Initialization",
    "section": "Glorot/Xavier init",
    "text": "Glorot/Xavier init\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]\n\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])\n\n\nThe result is nans everywhere. So maybe the scale of our matrix was too big, and we need to have smaller weights? But if we use too small weights, we will have the opposite problem—the scale of our activations will go from 1 to 0.1, and after 50 layers we’ll be left with zeros everywhere:\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.01)\nx[0:5,0:5]\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nSo we have to scale our weight matrices exactly right so that the standard deviation of our activations stays at 1. We can compute the exact value to use mathematically, as illustrated by Xavier Glorot and Yoshua Bengio in “Understanding the Difficulty of Training Deep Feedforward Neural Networks”. The right scale for a given layer is \\(1/\\sqrt{n_{in}}\\), where \\(n_{in}\\) represents the number of inputs.\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.1)\nx[0:5,0:5]\n\ntensor([[-0.81, -0.20, -0.18,  0.24, -0.47],\n        [ 0.04, -0.51,  0.52,  0.15,  0.53],\n        [ 0.10, -0.10, -0.35, -0.23, -0.68],\n        [ 0.03,  0.11, -1.07,  0.63,  0.05],\n        [-1.11, -0.25,  0.61, -0.83, -0.14]])\n\n\n\nBackground\n\n\nVariance and standard deviation\nVariance is the average of how far away each data point is from the mean. E.g.:\n\nt = torch.tensor([1.,2.,4.,18])\n\n\nm = t.mean(); m\n\ntensor(6.25)\n\n\n\n(t-m).mean()\n\ntensor(0.)\n\n\nOops. We can’t do that. Because by definition the positives and negatives cancel out. So we can fix that in one of (at least) two ways:\n\n(t-m).pow(2).mean()\n\ntensor(47.19)\n\n\n\n(t-m).abs().mean()\n\ntensor(5.88)\n\n\nBut the first of these is now a totally different scale, since we squared. So let’s undo that at the end.\n\n(t-m).pow(2).mean().sqrt()\n\ntensor(6.87)\n\n\nThey’re still different. Why?\nNote that we have one outlier (18). In the version where we square everything, it makes that much bigger than everything else.\n(t-m).pow(2).mean() is refered to as variance. It’s a measure of how spread out the data is, and is particularly sensitive to outliers.\nWhen we take the sqrt of the variance, we get the standard deviation. Since it’s on the same kind of scale as the original data, it’s generally more interpretable. However, since sqrt(1)==1, it doesn’t much matter which we use when talking about unit variance for initializing neural nets.\nThe standard deviation represents if the data stays close to the mean or on the contrary gets values that are far away. It’s computed by the following formula:\n\\[\\sigma = \\sqrt{\\frac{1}{n}\\left[(x_{0}-m)^{2} + (x_{1}-m)^{2} + \\cdots + (x_{n-1}-m)^{2}\\right]}\\]\nwhere m is the mean and \\(\\sigma\\) (the greek letter sigma) is the standard deviation. Here we have a mean of 0, so it’s just the square root of the mean of x squared.\n(t-m).abs().mean() is referred to as the mean absolute deviation. It isn’t used nearly as much as it deserves to be, because mathematicians don’t like how awkward it is to work with. But that shouldn’t stop us, because we have computers and stuff.\nHere’s a useful thing to note about variance:\n\n(t-m).pow(2).mean(), (t*t).mean() - (m*m)\n\n(tensor(47.19), tensor(47.19))\n\n\nYou can see why these are equal if you want to work thru the algebra. Or not.\nBut, what’s important here is that the latter is generally much easier to work with. In particular, you only have to track two things: the sum of the data, and the sum of squares of the data. Whereas in the first form you actually have to go thru all the data twice (once to calculate the mean, once to calculate the differences).\nLet’s go steal the LaTeX from Wikipedia:\n\\[\\operatorname{E}\\left[X^2 \\right] - \\operatorname{E}[X]^2\\]\n\n\nCovariance\nHere’s how Wikipedia defines covariance:\n\\[\\operatorname{cov}(X,Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}\\]\nLet’s see that in code. So now we need two vectors.\n\nt\n\ntensor([ 1.,  2.,  4., 18.])\n\n\n\n# `u` is twice `t`, plus a bit of randomness\nu = t*2\nu *= torch.randn_like(t)/10+0.95\n\nplt.scatter(t, u);\n\n\n\n\n\nprod = (t-t.mean())*(u-u.mean()); prod\n\ntensor([ 56.21,  35.17,   7.09, 260.07])\n\n\n\nprod.mean()\n\ntensor(89.64)\n\n\n\nv = torch.randn_like(t)\nplt.scatter(t, v);\n\n\n\n\n\n((t-t.mean())*(v-v.mean())).mean()\n\ntensor(-3.88)\n\n\nIt’s generally more conveniently defined like so:\n\\[\\operatorname{E}\\left[X Y\\right] - \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right]\\]\n\ncov = (t*v).mean() - t.mean()*v.mean(); cov\n\ntensor(-3.88)\n\n\nFrom now on, you’re not allowed to look at an equation (or especially type it in LaTeX) without also typing it in Python and actually calculating some values. Ideally, you should also plot some values.\nFinally, here is the Pearson correlation coefficient:\n\\[\\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y}\\]\n\ncov / (t.std() * v.std())\n\ntensor(-0.57)\n\n\nIt’s just a scaled version of the same thing.\n\nx.std()\n\ntensor(0.85)\n\n\n\n\nXavier init derivation\nWhen we do y = a @ x, the coefficients of y are defined by\n\\[y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + \\cdots + a_{i,n-1} x_{n-1} = \\sum_{k=0}^{n-1} a_{i,k} x_{k}\\]\nor in pure python code:\ny[i] = sum([c*d for c,d in zip(a[i], x)])\nor in numpy/pytorch code:\ny[i] = (a[i]*x).sum()\nAt the very beginning, our x vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way).\n\nx = torch.randn(100)\nx.mean(), x.std()\n\n(tensor(0.04), tensor(0.88))\n\n\nIf we go back to y = a @ x and assume that we chose weights for a that also have a mean of 0, we can compute the standard deviation of y quite easily. Since it’s random, and we may fall on bad numbers, we repeat the operation 100 times.\n\nmean,sqr = 0.,0.\nfor i in range(100):\n    x = torch.randn(100)\n    a = torch.randn(512, 100)\n    y = a @ x\n    mean += y.mean().item()\n    sqr  += y.pow(2).mean().item()\nmean/100,sqr/100\n\n(0.09304710745811462, 102.91469841003418)\n\n\nNow that looks very close to the dimension of our matrix 100. And that’s no coincidence! When you compute y, you sum 100 product of one element of a by one element of x. So what’s the mean and the standard deviation of such a product? We can show mathematically that as long as the elements in a and the elements in x are independent, the mean is 0 and the std is 1. This can also be seen experimentally:\n\nmean,sqr = 0.,0.\nfor i in range(10000):\n    x = torch.randn(1)\n    a = torch.randn(1)\n    y = a*x\n    mean += y.item()\n    sqr  += y.pow(2).item()\nmean/10000,sqr/10000\n\n(-0.002986055881790071, 0.9694275963147889)\n\n\nThen we sum 100 of those things that have a mean of zero, and a mean of squares of 1, so we get something that has a mean of 0, and mean of square of 100, hence math.sqrt(100) being our magic number. If we scale the weights of the matrix and divide them by this math.sqrt(100), it will give us a y of scale 1, and repeating the product has many times as we want won’t overflow or vanish."
  },
  {
    "objectID": "posts/course22p2/11_initializing.html#kaiminghe-init",
    "href": "posts/course22p2/11_initializing.html#kaiminghe-init",
    "title": "11 - Initialization",
    "section": "Kaiming/He init",
    "text": "Kaiming/He init\n(“He” is a Chinese surname and is pronouced like “Her”, not like “Hee”.)\n\nBackground\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\n\nfrom math import sqrt\n\n\nw1 = torch.randn(100,50) / sqrt(100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) / sqrt(50)\nb2 = torch.zeros(1)\n\n\ndef lin(x, w, b): return x @ w + b\n\n\nl1 = lin(x, w1, b1)\nl1.mean(),l1.std()\n\n(tensor(0.02), tensor(0.98))\n\n\n\ndef relu(x): return x.clamp_min(0.)\n\n\nl2 = relu(l1)\nl2.mean(),l2.std()\n\n(tensor(0.40), tensor(0.58))\n\n\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1))\nx[0:5,0:5]\n\ntensor([[    0.00,     0.00,     0.00,     0.00,     0.00],\n        [    0.00,     0.00,     0.00,     0.00,     0.00],\n        [    0.00,     0.00,     0.00,     0.00,     0.00],\n        [    0.00,     0.00,     0.00,     0.00,     0.00],\n        [    0.00,     0.00,     0.00,     0.00,     0.00]])\n\n\nIn “Delving Deep into Rectifiers: Surpassing Human-Level Performance” Kaiming He et al. show that we should use the following scale instead: \\(\\sqrt{2 / n_{in}}\\), where \\(n_{in}\\) is the number of inputs of our model.\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\nx[0:5,0:5]\n\ntensor([[0.00, 0.00, 0.00, 0.07, 0.00],\n        [0.00, 0.00, 0.00, 0.01, 0.00],\n        [0.07, 0.00, 0.00, 0.06, 0.00],\n        [0.00, 0.00, 0.00, 0.15, 0.00],\n        [0.02, 0.00, 0.00, 0.00, 0.00]])\n\n\n\n\nApplying an init function\n\nmodel = get_model()\nmodel.apply(lambda m: print(type(m).__name__));\n\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nFlatten\nSequential\n\n\n\ndef init_weights(m):\n    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): init.kaiming_normal_(m.weight)\n\n\nmodel.apply(init_weights);\n\n\nMomentumLearner(model, dls, F.cross_entropy, cbs=[DeviceCB()]).lr_find()\n\n\n\n\n\nset_seed(42)\nlearn = MomentumLearner(get_model().apply(init_weights), dls, F.cross_entropy, lr=0.2, cbs=cbs)\n\n\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.174\n      2.353\n      0\n      train\n    \n    \n      0.100\n      2.303\n      0\n      eval\n    \n    \n      0.100\n      2.303\n      1\n      train\n    \n    \n      0.100\n      2.303\n      1\n      eval\n    \n    \n      0.100\n      2.303\n      2\n      train\n    \n    \n      0.100\n      2.303\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\nastats.plot_stats()"
  },
  {
    "objectID": "posts/course22p2/11_initializing.html#input-normalization",
    "href": "posts/course22p2/11_initializing.html#input-normalization",
    "title": "11 - Initialization",
    "section": "Input normalization",
    "text": "Input normalization\n\nxmean,xstd = xb.mean(),xb.std()\nxmean,xstd\n\n(tensor(0.28), tensor(0.35))\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass BatchTransformCB(Callback):\n    def __init__(self, tfm, on_train=True, on_val=True): fc.store_attr()\n\n    def before_batch(self, learn):\n        if (self.on_train and learn.training) or (self.on_val and not learn.training):\n            learn.batch = self.tfm(learn.batch)\n:::\n\ndef _norm(b): return (b[0]-xmean)/xstd,b[1]\nnorm = BatchTransformCB(_norm)\n\n\nset_seed(42)\nlearn = MomentumLearner(get_model().apply(init_weights), dls, F.cross_entropy, lr=0.2, cbs=cbs+[norm])\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.669\n      0.977\n      0\n      train\n    \n    \n      0.812\n      0.509\n      0\n      eval\n    \n    \n      0.842\n      0.425\n      1\n      train\n    \n    \n      0.843\n      0.428\n      1\n      eval\n    \n    \n      0.860\n      0.376\n      2\n      train\n    \n    \n      0.853\n      0.399\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\nastats.plot_stats()\n\n\n\n\n\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\nxb,yb = next(iter(dls.train))\n\n\nxb.mean(),xb.std()\n\n(tensor(    0.00), tensor(1.))"
  },
  {
    "objectID": "posts/course22p2/11_initializing.html#general-relu",
    "href": "posts/course22p2/11_initializing.html#general-relu",
    "title": "11 - Initialization",
    "section": "General ReLU",
    "text": "General ReLU\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass GeneralRelu(nn.Module):\n    def __init__(self, leak=None, sub=None, maxv=None):\n        super().__init__()\n        self.leak,self.sub,self.maxv = leak,sub,maxv\n\n    def forward(self, x): \n        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n        if self.sub is not None: x -= self.sub\n        if self.maxv is not None: x.clamp_max_(self.maxv)\n        return x\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef plot_func(f, start=-5., end=5., steps=100):\n    x = torch.linspace(start, end, steps)\n    plt.plot(x, f(x))\n    plt.grid(True, which='both', ls='--')\n    plt.axhline(y=0, color='k', linewidth=0.7)\n    plt.axvline(x=0, color='k', linewidth=0.7)\n:::\n\nplot_func(GeneralRelu(leak=0.1, sub=0.4))\n\n\n\n\n\ndef conv(ni, nf, ks=3, stride=2, act=nn.ReLU):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, act())\n    return res\n\n\ndef get_model(act=nn.ReLU, nfs=None):\n    if nfs is None: nfs = [1,8,16,32,64]\n    layers = [conv(nfs[i], nfs[i+1], act=act) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None), nn.Flatten()).to(def_device)\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef init_weights(m, leaky=0.):\n    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): init.kaiming_normal_(m.weight, a=leaky)\n:::\n\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\niw = partial(init_weights, leaky=0.1)\n\n\nmodel = get_model(act_gr).apply(iw)\n\n\nset_seed(42)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.760\n      0.690\n      0\n      train\n    \n    \n      0.846\n      0.423\n      0\n      eval\n    \n    \n      0.866\n      0.365\n      1\n      train\n    \n    \n      0.860\n      0.373\n      1\n      eval\n    \n    \n      0.883\n      0.319\n      2\n      train\n    \n    \n      0.872\n      0.350\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\nastats.plot_stats()\n\n\n\n\n\nastats.dead_chart()"
  },
  {
    "objectID": "posts/course22p2/11_initializing.html#lsuv",
    "href": "posts/course22p2/11_initializing.html#lsuv",
    "title": "11 - Initialization",
    "section": "LSUV",
    "text": "LSUV\nAll You Need is a Good Init introduces Layer-wise Sequential Unit-Variance (LSUV).\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef _lsuv_stats(hook, mod, inp, outp):\n    acts = to_cpu(outp)\n    hook.mean = acts.mean()\n    hook.std = acts.std()\n\ndef lsuv_init(model, m, m_in, xb):\n    h = Hook(m, _lsuv_stats)\n    with torch.no_grad():\n        while model(xb) is not None and (abs(h.std-1)>1e-3 or abs(h.mean)>1e-3):\n            m_in.bias -= h.mean\n            m_in.weight.data /= h.std\n    h.remove()\n:::\n\nmodel = get_model(act_gr)\nrelus = [o for o in model.modules() if isinstance(o, GeneralRelu)]\nconvs = [o for o in model.modules() if isinstance(o, nn.Conv2d)]\n\n\nfor ms in zip(relus,convs): print(ms)\n\n(GeneralRelu(), Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n(GeneralRelu(), Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n(GeneralRelu(), Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n(GeneralRelu(), Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n\n\n\nfor ms in zip(relus,convs): lsuv_init(model, *ms, xb.to(def_device))\n\n\nset_seed(42)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.764\n      0.667\n      0\n      train\n    \n    \n      0.845\n      0.439\n      0\n      eval\n    \n    \n      0.863\n      0.375\n      1\n      train\n    \n    \n      0.861\n      0.392\n      1\n      eval\n    \n    \n      0.877\n      0.337\n      2\n      train\n    \n    \n      0.866\n      0.378\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nastats.plot_stats()"
  },
  {
    "objectID": "posts/course22p2/11_initializing.html#batch-normalization",
    "href": "posts/course22p2/11_initializing.html#batch-normalization",
    "title": "11 - Initialization",
    "section": "Batch Normalization",
    "text": "Batch Normalization\nSergey Ioffe and Christian Szegedy released “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” in 2015, saying:\n\nTraining Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization… We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.\n\nTheir proposal is:\n\nMaking normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization.\n\n\nLayerNorm\nWe’ll start with layer normalization, a simpler technique.\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dummy, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.mult = nn.Parameter(tensor(1.))\n        self.add  = nn.Parameter(tensor(0.))\n\n    def forward(self, x):\n        m = x.mean((1,2,3), keepdim=True)\n        v = x.var ((1,2,3), keepdim=True)\n        x = (x-m) / ((v+self.eps).sqrt())\n        return x*self.mult + self.add\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef conv(ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n    if bias is None: bias = not isinstance(norm, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d))\n    layers = [nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias)]\n    if norm: layers.append(norm(nf))\n    if act: layers.append(act())\n    return nn.Sequential(*layers)\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef get_model(act=nn.ReLU, nfs=None, norm=None):\n    if nfs is None: nfs = [1,8,16,32,64]\n    layers = [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None, norm=False, bias=True),\n                         nn.Flatten()).to(def_device)\n:::\n\nset_seed(42)\nmodel = get_model(act_gr, norm=LayerNorm).apply(iw)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.779\n      0.616\n      0\n      train\n    \n    \n      0.844\n      0.422\n      0\n      eval\n    \n    \n      0.866\n      0.369\n      1\n      train\n    \n    \n      0.866\n      0.369\n      1\n      eval\n    \n    \n      0.882\n      0.322\n      2\n      train\n    \n    \n      0.874\n      0.346\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\n\nBatchNorm\n\nclass BatchNorm(nn.Module):\n    def __init__(self, nf, mom=0.1, eps=1e-5):\n        super().__init__()\n        # NB: pytorch bn mom is opposite of what you'd expect\n        self.mom,self.eps = mom,eps\n        self.mults = nn.Parameter(torch.ones (nf,1,1))\n        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n        self.register_buffer('vars',  torch.ones(1,nf,1,1))\n        self.register_buffer('means', torch.zeros(1,nf,1,1))\n\n    def update_stats(self, x):\n        m = x.mean((0,2,3), keepdim=True)\n        v = x.var ((0,2,3), keepdim=True)\n        self.means.lerp_(m, self.mom)\n        self.vars.lerp_ (v, self.mom)\n        return m,v\n        \n    def forward(self, x):\n        if self.training:\n            with torch.no_grad(): m,v = self.update_stats(x)\n        else: m,v = self.means,self.vars\n        x = (x-m) / (v+self.eps).sqrt()\n        return x*self.mults + self.adds\n\n\nmodel = get_model(act_gr, norm=BatchNorm).apply(iw)\nset_seed(42)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.4, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.797\n      0.554\n      0\n      train\n    \n    \n      0.822\n      0.543\n      0\n      eval\n    \n    \n      0.873\n      0.346\n      1\n      train\n    \n    \n      0.855\n      0.400\n      1\n      eval\n    \n    \n      0.887\n      0.307\n      2\n      train\n    \n    \n      0.849\n      0.416\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\n\n\nVarious norms"
  },
  {
    "objectID": "posts/course22p2/11_initializing.html#towards-90",
    "href": "posts/course22p2/11_initializing.html#towards-90",
    "title": "11 - Initialization",
    "section": "Towards 90%…",
    "text": "Towards 90%…\n\ndls = DataLoaders.from_dd(tds, 256, num_workers=4)\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.834\n      0.452\n      0\n      train\n    \n    \n      0.848\n      0.417\n      0\n      eval\n    \n    \n      0.882\n      0.318\n      1\n      train\n    \n    \n      0.866\n      0.377\n      1\n      eval\n    \n    \n      0.894\n      0.283\n      2\n      train\n    \n    \n      0.876\n      0.344\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.05, cbs=cbs)\nlearn.fit(2)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.916\n      0.227\n      0\n      train\n    \n    \n      0.897\n      0.284\n      0\n      eval\n    \n    \n      0.922\n      0.213\n      1\n      train\n    \n    \n      0.899\n      0.282\n      1\n      eval"
  },
  {
    "objectID": "posts/course22p2/11_initializing.html#export--",
    "href": "posts/course22p2/11_initializing.html#export--",
    "title": "11 - Initialization",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "posts/course22p2/08_autoencoder.html",
    "href": "posts/course22p2/08_autoencoder.html",
    "title": "08 - Autoencoders and how manual coding doesn’t work",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nI this notebook is for explaning why and how manual coding doesnt work. watch the video and skip to the next one. (notice there is no exports here)"
  },
  {
    "objectID": "posts/course22p2/08_autoencoder.html#data",
    "href": "posts/course22p2/08_autoencoder.html#data",
    "title": "08 - Autoencoders and how manual coding doesn’t work",
    "section": "Data",
    "text": "Data\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name, ignore_verifications=True)\n\n\n\n\n\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\n\nbs = 256\ntds = dsd.with_transform(transformi)\n\n\nds = tds['train']\nimg = ds[0]['image']\nshow_image(img, figsize=(1,1));\n\n\n\n\n\ncf = collate_dict(ds)\n\n\ndef collate_(b): return to_device(cf(b))\ndef data_loaders(dsd, bs, **kwargs): return {k:DataLoader(v, bs, **kwargs) for k,v in dsd.items()}\n\n\ndls = data_loaders(tds, bs, collate_fn=collate_)\n\n\ndt = dls['train']\ndv = dls['test']\n\nxb,yb = next(iter(dt))\n\n\nlabels = ds.features[y].names\n\n\nlabels\n\n['T - shirt / top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']\n\n\n\nlbl_getter = itemgetter(*yb[:16])\ntitles = lbl_getter(labels)\n\n\nmpl.rcParams['figure.dpi'] = 70\nshow_images(xb[:16], imsize=1.7, titles=titles)"
  },
  {
    "objectID": "posts/course22p2/08_autoencoder.html#warmup---classify",
    "href": "posts/course22p2/08_autoencoder.html#warmup---classify",
    "title": "08 - Autoencoders and how manual coding doesn’t work",
    "section": "Warmup - classify",
    "text": "Warmup - classify\n\nfrom torch import optim\n\nbs = 256\nlr = 0.4\n\n\ncnn = nn.Sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,16),           #2x2\n    conv(16,10, act=False),\n    nn.Flatten()).to(def_device)\n\n\nopt = optim.SGD(cnn.parameters(), lr=lr)\nloss,acc = fit(5, cnn, F.cross_entropy, opt, dt, dv)\n\n0 0.37266189764738084 0.8627\n1 0.36386815688610075 0.866\n2 0.3640298747062683 0.8672\n3 0.36340640761852266 0.8692\n4 0.36365662859678266 0.8698\n\n\n\ndsd['train'][0]\n\n{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n 'label': 9}"
  },
  {
    "objectID": "posts/course22p2/08_autoencoder.html#autoencoder",
    "href": "posts/course22p2/08_autoencoder.html#autoencoder",
    "title": "08 - Autoencoders and how manual coding doesn’t work",
    "section": "Autoencoder",
    "text": "Autoencoder\n\ndef deconv(ni, nf, ks=3, act=True):\n    layers = [nn.UpsamplingNearest2d(scale_factor=2),\n              nn.Conv2d(ni, nf, stride=1, kernel_size=ks, padding=ks//2)]\n    if act: layers.append(nn.ReLU())\n    return nn.Sequential(*layers)\n\n\ndef eval(model, loss_func, valid_dl, epoch=0):\n    model.eval()\n    with torch.no_grad():\n        tot_loss,count = 0.,0\n        for xb,_ in valid_dl:\n            pred = model(xb)\n            n = len(xb)\n            count += n\n            tot_loss += loss_func(pred,xb).item()*n\n    print(epoch, f'{tot_loss/count:.3f}')\n\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb,_ in train_dl:\n            loss = loss_func(model(xb), xb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        eval(model, loss_func, valid_dl, epoch)\n\n\nae = nn.Sequential(   #28x28\n    nn.ZeroPad2d(2),  #32x32\n    conv(1,2),        #16x16\n    conv(2,4),        #8x8\n#     conv(4,8),        #4x4\n#     deconv(8,4),      #8x8\n    deconv(4,2),      #16x16\n    deconv(2,1, act=False), #32x32\n    nn.ZeroPad2d(-2), #28x28\n    nn.Sigmoid()\n).to(def_device)\n\n\neval(ae, F.mse_loss, dv)\n\n0 0.158\n\n\n\nopt = optim.SGD(ae.parameters(), lr=0.01)\nfit(5, ae, F.mse_loss, opt, dt, dv)\n\n0 0.136\n1 0.127\n2 0.125\n3 0.124\n4 0.124\n\n\n\nopt = optim.SGD(ae.parameters(), lr=0.1)\nfit(5, ae, F.mse_loss, opt, dt, dv)\n\n0 0.108\n1 0.047\n2 0.039\n3 0.036\n4 0.034\n\n\n\np = ae(xb)\nshow_images(p[:16].data.cpu(), imsize=1.5)\n\n\n\n\n\np = ae(xb)\nshow_images(p[:16].data.cpu(), imsize=1.5)\n\n\n\n\n\nshow_images(xb[:16].data.cpu(), imsize=1.5)"
  },
  {
    "objectID": "posts/course22p2/07_convolutions.html",
    "href": "posts/course22p2/07_convolutions.html",
    "title": "07 - Convolutions",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page."
  },
  {
    "objectID": "posts/course22p2/07_convolutions.html#creating-the-cnn",
    "href": "posts/course22p2/07_convolutions.html#creating-the-cnn",
    "title": "07 - Convolutions",
    "section": "Creating the CNN",
    "text": "Creating the CNN\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nbroken_cnn = nn.Sequential(\n    nn.Conv2d(1,30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30,10, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([16, 10, 28, 28])\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef conv(ni, nf, ks=3, stride=2, act=True):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n:::\nRefactoring parts of your neural networks like this makes it much less likely you’ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing.\n\nsimple_cnn = nn.Sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,16),           #2x2\n    conv(16,10, act=False), #1x1\n    nn.Flatten(),\n)\n\n\nsimple_cnn(xb).shape\n\ntorch.Size([16, 10])\n\n\n\nx_imgs = x_train.view(-1,1,28,28)\nxv_imgs = x_valid.view(-1,1,28,28)\ntrain_ds,valid_ds = Dataset(x_imgs, y_train),Dataset(xv_imgs, y_valid)\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef to_device(x, device=def_device):\n    if isinstance(x, torch.Tensor): return x.to(device)\n    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n    return type(x)(to_device(o, device) for o in x)\n\ndef collate_device(b): return to_device(default_collate(b))\n:::\n\nfrom torch import optim\n\nbs = 256\nlr = 0.4\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\nopt = optim.SGD(simple_cnn.parameters(), lr=lr)\n\n\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\n0 0.3630618950843811 0.8875999997138977\n1 0.16439641580581665 0.9496000003814697\n2 0.24622697901725768 0.9316000004768371\n3 0.25093305287361145 0.9335999998092651\n4 0.13128829071521758 0.9618000007629395\n\n\n\nopt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\n0 0.08451943595409393 0.9756999996185303\n1 0.08082638642787933 0.9777999995231629\n2 0.08050601842403411 0.9778999995231629\n3 0.08200360851287841 0.9773999995231628\n4 0.08405050563812255 0.9761999994277955\n\n\n\nUnderstanding Convolution Arithmetic\nIn an input of size 64x1x28x28 the axes are batch,channel,height,width. This is often represented as NCHW (where N refers to batch size). Tensorflow, on the other hand, uses NHWC axis order (aka “channels-last”). Channels-last is faster for many models, so recently it’s become more common to see this as an option in PyTorch too.\nWe have 1 input channel, 4 output channels, and a 3×3 kernel.\n\nsimple_cnn[0][0]\n\nConv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n\n\nconv1 = simple_cnn[0][0]\nconv1.weight.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\n\nconv1.bias.shape\n\ntorch.Size([4])\n\n\nThe receptive field is the area of an image that is involved in the calculation of a layer. conv-example.xlsx shows the calculation of two stride-2 convolutional layers using an MNIST digit. Here’s what we see if we click on one of the cells in the conv2 section, which shows the output of the second convolutional layer, and click trace precedents.\n\nThe blue highlighted cells are its precedents—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Click trace precedents again:\n\nIn this example, we have just two convolutional layers. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This is the receptive field\nThe deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer."
  },
  {
    "objectID": "posts/course22p2/07_convolutions.html#color-images",
    "href": "posts/course22p2/07_convolutions.html#color-images",
    "title": "07 - Convolutions",
    "section": "Color Images",
    "text": "Color Images\nA colour picture is a rank-3 tensor:\n\nfrom torchvision.io import read_image\n\n\nim = read_image('images/grizzly.jpg')\nim.shape\n\ntorch.Size([3, 1000, 846])\n\n\n\nshow_image(im.permute(1,2,0));\n\n\n\n\n\n_,axs = plt.subplots(1,3)\nfor bear,ax,color in zip(im,axs,('Reds','Greens','Blues')): show_image(255-bear, ax=ax, cmap=color)\n\n\n\n\n\nThese are then all added together, to produce a single number, for each grid location, for each output feature.\n\nWe have ch_out filters like this, so in the end, the result of our convolutional layer will be a batch of images with ch_out channels."
  },
  {
    "objectID": "posts/course22p2/07_convolutions.html#export--",
    "href": "posts/course22p2/07_convolutions.html#export--",
    "title": "07 - Convolutions",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html",
    "href": "posts/course22p2/04_minibatch_training.html",
    "title": "04 - Minibatch Training",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nEverything from scratch to pytorch\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#initial-setup",
    "href": "posts/course22p2/04_minibatch_training.html#initial-setup",
    "title": "04 - Minibatch Training",
    "section": "Initial setup",
    "text": "Initial setup\n\nData\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nmodel = Model(m, nh, 10)\npred = model(x_train)\npred.shape\n\ntorch.Size([50000, 10])\n\n\n\n\nCross entropy loss\nFirst, we will need to compute the softmax of our activations. This is defined by:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}\\]\nor more concisely:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}\\]\nIn practice, we will need the log of the softmax when we calculate the loss.\n\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\n\nlog_softmax(pred)\n\ntensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n        ...,\n        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)\n\n\nNote that the formula\n\\[\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)\\]\ngives a simplification when we compute the log softmax:\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\n\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )\\]\nwhere a is the maximum of the \\(x_{j}\\).\n\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\nThis way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us.\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\ntest_close(logsumexp(pred), pred.logsumexp(-1))\nsm_pred = log_softmax(pred)\nsm_pred\n\ntensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n        ...,\n        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)\n\n\nThe cross entropy loss for some target \\(x\\) and some prediction \\(p(x)\\) is given by:\n\\[ -\\sum x\\, \\log p(x) \\]\nBut since our \\(x\\)s are 1-hot encoded (actually, they’re just the integer indices), this can be rewritten as \\(-\\log(p_{i})\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\n\n\n\n\n\ncheck your model for this trick\n\n\n\nhttps://www.youtube.com/watch?v=vGdB4eI4KBs 1:38\n\n\n\ny_train[:3]\n\ntensor([5, 0, 4])\n\n\n\nsm_pred[0,5],sm_pred[1,0],sm_pred[2,4]\n\n(tensor(-2.20, grad_fn=<SelectBackward0>),\n tensor(-2.37, grad_fn=<SelectBackward0>),\n tensor(-2.36, grad_fn=<SelectBackward0>))\n\n\n\nsm_pred[[0,1,2], y_train[:3]]\n\ntensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)\n\n\n\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\n\nloss = nll(sm_pred, y_train)\nloss\n\ntensor(2.30, grad_fn=<NegBackward0>)\n\n\nThen use PyTorch’s implementation.\n\ntest_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)\n\nIn PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy.\n\ntest_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#basic-training-loop",
    "href": "posts/course22p2/04_minibatch_training.html#basic-training-loop",
    "title": "04 - Minibatch Training",
    "section": "Basic training loop",
    "text": "Basic training loop\nBasically the training loop repeats over the following steps: - get the output of the model on a batch of inputs - compare the output to the labels we have and compute a loss - calculate the gradients of the loss with respect to every parameter of the model - update said parameters with those gradients to make them a little bit better\n\nloss_func = F.cross_entropy\n\n\nbs=50                  # batch size\n\nxb = x_train[0:bs]     # a mini-batch from x\npreds = model(xb)      # predictions\npreds[0], preds.shape\n\n(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n torch.Size([50, 10]))\n\n\n\nyb = y_train[0:bs]\nyb\n\ntensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n        3, 9, 8, 5, 9, 3])\n\n\n\nloss_func(preds, yb)\n\ntensor(2.30, grad_fn=<NllLossBackward0>)\n\n\n\npreds.argmax(dim=1)\n\ntensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8,\n        3, 5, 9, 5, 9, 5])\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()\n:::\n\naccuracy(preds, yb)\n\ntensor(0.08)\n\n\n\nlr = 0.5   # learning rate\nepochs = 3 # how many epochs to train for\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')\n:::\n\nxb,yb = x_train[:bs],y_train[:bs]\npreds = model(xb)\nreport(loss_func(preds, yb), preds, yb)\n\n2.30, 0.08\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, 'weight'):\n                    l.weight -= l.weight.grad * lr\n                    l.bias   -= l.bias.grad   * lr\n                    l.weight.grad.zero_()\n                    l.bias  .grad.zero_()\n    report(loss, preds, yb)\n\n0.12, 0.98\n0.12, 0.94\n0.08, 0.96"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#using-parameters-and-optim",
    "href": "posts/course22p2/04_minibatch_training.html#using-parameters-and-optim",
    "title": "04 - Minibatch Training",
    "section": "Using parameters and optim",
    "text": "Using parameters and optim\n\nParameters\n\nm1 = nn.Module()\nm1.foo = nn.Linear(3,4)\nm1\n\nModule(\n  (foo): Linear(in_features=3, out_features=4, bias=True)\n)\n\n\n\nlist(m1.named_children())\n\n[('foo', Linear(in_features=3, out_features=4, bias=True))]\n\n\n\nm1.named_children()\n\n<generator object Module.named_children>\n\n\n\nlist(m1.parameters())\n\n[Parameter containing:\n tensor([[ 0.57,  0.43, -0.30],\n         [ 0.13, -0.32, -0.24],\n         [ 0.51,  0.04,  0.22],\n         [ 0.13, -0.17, -0.24]], requires_grad=True),\n Parameter containing:\n tensor([-0.01, -0.51, -0.39,  0.56], requires_grad=True)]\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.l1 = nn.Linear(n_in,nh)\n        self.l2 = nn.Linear(nh,n_out)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x): return self.l2(self.relu(self.l1(x)))\n\n\nmodel = MLP(m, nh, 10)\nmodel.l1\n\nLinear(in_features=784, out_features=50, bias=True)\n\n\n\nmodel\n\nMLP(\n  (l1): Linear(in_features=784, out_features=50, bias=True)\n  (l2): Linear(in_features=50, out_features=10, bias=True)\n  (relu): ReLU()\n)\n\n\n\nfor name,l in model.named_children(): print(f\"{name}: {l}\")\n\nl1: Linear(in_features=784, out_features=50, bias=True)\nl2: Linear(in_features=50, out_features=10, bias=True)\nrelu: ReLU()\n\n\n\nfor p in model.parameters(): print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n\ndef fit():\n    for epoch in range(epochs):\n        for i in range(0, n, bs):\n            s = slice(i, min(n,i+bs))\n            xb,yb = x_train[s],y_train[s]\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters(): p -= p.grad * lr\n                model.zero_grad()\n        report(loss, preds, yb)\n\n\nfit()\n\n0.19, 0.96\n0.11, 0.96\n0.04, 1.00\n\n\nBehind the scenes, PyTorch overrides the __setattr__ function in nn.Module so that the submodules you define are properly registered as parameters of the model.\n\nhow to create a module is below. kw = create module\n\nclass MyModule:\n    def __init__(self, n_in, nh, n_out):\n        self._modules = {}\n        self.l1 = nn.Linear(n_in,nh)\n        self.l2 = nn.Linear(nh,n_out)\n\n    def __setattr__(self,k,v):\n        if not k.startswith(\"_\"): self._modules[k] = v\n        super().__setattr__(k,v)\n\n    def __repr__(self): return f'{self._modules}'\n    \n    def parameters(self):\n        for l in self._modules.values(): yield from l.parameters()\n\n\nmdl = MyModule(m,nh,10)\nmdl\n\n{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}\n\n\n\nfor p in mdl.parameters(): print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n\n\nRegistering modules\n\nfrom functools import reduce\n\nWe can use the original layers approach, but we have to register the modules.\n\nlayers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]\n\n\n\n\n\n\n\nwhat is reduce\n\n\n\nreduce(lambda val,layer: layer(val), self.layers, x) is same as\nfor l in self.layers: x = l(x)\ncheck below\nclass SequentialModel(nn.Module):\n\n\n\nclass Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = layers\n        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n\n    def forward(self, x): return reduce(lambda val,layer: layer(val), self.layers, x)\n\n\nmodel = Model(layers)\nmodel\n\nModel(\n  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n  (layer_1): ReLU()\n  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nmodel(xb).shape\n\ntorch.Size([50, 10])\n\n\n\n\nnn.ModuleList\nnn.ModuleList does this for us.\n\nclass SequentialModel(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        \n    def forward(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nmodel = SequentialModel(layers)\nmodel\n\nSequentialModel(\n  (layers): ModuleList(\n    (0): Linear(in_features=784, out_features=50, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=50, out_features=10, bias=True)\n  )\n)\n\n\n\nfit()\n\n0.12, 0.96\n0.11, 0.96\n0.07, 0.98\n\n\n\n\nnn.Sequential\nnn.Sequential is a convenient class which does the same as the above:\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.16, 0.94\n0.13, 0.96\n0.08, 0.96\n\n\n(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))\n\n\n\nmodel\n\nSequential(\n  (0): Linear(in_features=784, out_features=50, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\n\noptim\n\nclass Optimizer():\n    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n\n    def step(self):\n        with torch.no_grad():\n            for p in self.params: p -= p.grad * self.lr\n\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nopt = Optimizer(model.parameters())\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.18, 0.94\n0.13, 0.96\n0.11, 0.94\n\n\nPyTorch already provides this exact functionality in optim.SGD (it also handles stuff like momentum, which we’ll look at later)\n\nfrom torch import optim\n\n\ndef get_model():\n    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n    return model, optim.SGD(model.parameters(), lr=lr)\n\n\nmodel,opt = get_model()\nloss_func(model(xb), yb)\n\ntensor(2.33, grad_fn=<NllLossBackward0>)\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.12, 0.98\n0.09, 0.98\n0.07, 0.98"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#dataset-and-dataloader",
    "href": "posts/course22p2/04_minibatch_training.html#dataset-and-dataloader",
    "title": "04 - Minibatch Training",
    "section": "Dataset and DataLoader",
    "text": "Dataset and DataLoader\n\nDataset\nIt’s clunky to iterate through minibatches of x and y values separately:\n    xb = x_train[s]\n    yb = y_train[s]\nInstead, let’s do these two steps together, by introducing a Dataset class:\n    xb,yb = train_ds[s]\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass Dataset():\n    def __init__(self, x, y): self.x,self.y = x,y\n    def __len__(self): return len(self.x)\n    def __getitem__(self, i): return self.x[i],self.y[i]\n:::\n\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\nassert len(train_ds)==len(x_train)\nassert len(valid_ds)==len(x_valid)\n\n\nxb,yb = train_ds[0:5]\nassert xb.shape==(5,28*28)\nassert yb.shape==(5,)\nxb,yb\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([5, 0, 4, 1, 9]))\n\n\n\nmodel,opt = get_model()\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        xb,yb = train_ds[i:min(n,i+bs)]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.17, 0.96\n0.11, 0.94\n0.09, 0.96\n\n\n\n\nDataLoader\nPreviously, our loop iterated over batches (xb, yb) like this:\nfor i in range(0, n, bs):\n    xb,yb = train_ds[i:min(n,i+bs)]\n    ...\nLet’s make our loop much cleaner, using a data loader:\nfor xb,yb in train_dl:\n    ...\n\nclass DataLoader():\n    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n    def __iter__(self):\n        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]\n\n\ntrain_dl = DataLoader(train_ds, bs)\nvalid_dl = DataLoader(valid_ds, bs)\n\n\nxb,yb = next(iter(valid_dl))\nxb.shape\n\ntorch.Size([50, 784])\n\n\n\nyb\n\ntensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n        8, 3, 7, 7, 8, 4])\n\n\n\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\nmodel,opt = get_model()\n\n\ndef fit():\n    for epoch in range(epochs):\n        for xb,yb in train_dl:\n            pred = model(xb)\n            loss = loss_func(pred, yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        report(loss, preds, yb)\n\n\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.11, 0.96\n0.09, 0.96\n0.06, 0.96\n\n\n(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))\n\n\n\n\nRandom sampling\nWe want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn’t be randomized.\n\nimport random\n\n\nclass Sampler():\n    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n    def __iter__(self):\n        res = list(range(self.n))\n        if self.shuffle: random.shuffle(res)\n        return iter(res)\n\n\nfrom itertools import islice\n\n\nss = Sampler(train_ds)\n\n\nit = iter(ss)\nfor o in range(5): print(next(it))\n\n0\n1\n2\n3\n4\n\n\n\nlist(islice(ss, 5))\n\n[0, 1, 2, 3, 4]\n\n\n\nss = Sampler(train_ds, shuffle=True)\nlist(islice(ss, 5))\n\n[11815, 32941, 21760, 21778, 35233]\n\n\n\nimport fastcore.all as fc\n\n\nclass BatchSampler():\n    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)\n\n\nbatchs = BatchSampler(ss, 4)\nlist(islice(batchs, 5))\n\n[[30214, 5339, 461, 9948],\n [8032, 20805, 16282, 13099],\n [26751, 2761, 552, 12897],\n [16714, 7294, 34658, 24330],\n [13836, 28629, 16552, 32028]]\n\n\n\ndef collate(b):\n    xs,ys = zip(*b)\n    return torch.stack(xs),torch.stack(ys)\n\n\nclass DataLoader():\n    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)\n\n\ntrain_samp = BatchSampler(Sampler(train_ds, shuffle=True ), bs)\nvalid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_samp)\nvalid_dl = DataLoader(valid_ds, batchs=valid_samp)\n\n\nxb,yb = next(iter(valid_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\nxb.shape,yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\nmodel,opt = get_model()\n\n\nfit()\n\n0.16, 0.08\n0.08, 0.04\n0.06, 0.08\n\n\n\n\nMultiprocessing DataLoader\n\nimport torch.multiprocessing as mp\nfrom fastcore.basics import store_attr\n\n\ntrain_ds[[3,6,8,1]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 1, 1, 0]))\n\n\n\ntrain_ds.__getitem__([3,6,8,1])\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 1, 1, 0]))\n\n\n\nfor o in map(train_ds.__getitem__, ([3,6],[8,1])): print(o)\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n\n\n\nclass DataLoader():\n    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n    def __iter__(self):\n        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\nit = iter(train_dl)\n\n\nxb,yb = next(it)\nxb.shape,yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\n\nPyTorch DataLoader\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n:::\n\ntrain_samp = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\nvalid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)\n\n\ntrain_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)\n\n\nmodel,opt = get_model()\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.10, 0.06\n0.10, 0.04\n0.27, 0.06\n\n\n(tensor(0.25, grad_fn=<NllLossBackward0>), tensor(0.94))\n\n\nPyTorch can auto-generate the BatchSampler for us:\n\ntrain_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)\n\nPyTorch can also generate the Sequential/RandomSamplers too:\n\ntrain_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\nvalid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)\n\n\nmodel,opt = get_model()\nfit()\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.21, 0.14\n0.15, 0.16\n0.05, 0.10\n\n\n(tensor(0.22, grad_fn=<NllLossBackward0>), tensor(0.96))\n\n\nOur dataset actually already knows how to sample a batch of indices all at once:\n\ntrain_ds[[4,6,7]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([9, 1, 3]))\n\n\n…that means that we can actually skip the batch_sampler and collate_fn entirely:\n\ntrain_dl = DataLoader(train_ds, sampler=train_samp)\nvalid_dl = DataLoader(valid_ds, sampler=valid_samp)\n\n\nxb,yb = next(iter(train_dl))\nxb.shape,yb.shape\n\n(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#validation",
    "href": "posts/course22p2/04_minibatch_training.html#validation",
    "title": "04 - Minibatch Training",
    "section": "Validation",
    "text": "Validation\nYou always should also have a validation set, in order to identify if you are overfitting.\nWe will calculate and print the validation loss at the end of each epoch.\n(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb,yb in train_dl:\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n        with torch.no_grad():\n            tot_loss,tot_acc,count = 0.,0.,0\n            for xb,yb in valid_dl:\n                pred = model(xb)\n                n = len(xb)\n                count += n\n                tot_loss += loss_func(pred,yb).item()*n\n                tot_acc  += accuracy (pred,yb).item()*n\n        print(epoch, tot_loss/count, tot_acc/count)\n    return tot_loss/count, tot_acc/count\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef get_dls(train_ds, valid_ds, bs, **kwargs):\n    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n:::\nNow, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:\n\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\nmodel,opt = get_model()\n\n\n%time loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)\n\n0 0.14236384611576797 0.958100004196167\n1 0.12564025789499284 0.9632000041007995\n2 0.1306914868950844 0.9645000052452087\n3 0.10988455526065082 0.9670000064373017\n4 0.11636362857650966 0.9678000068664551\nCPU times: user 10.5 s, sys: 16.3 s, total: 26.8 s\nWall time: 1.68 s"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#export--",
    "href": "posts/course22p2/04_minibatch_training.html#export--",
    "title": "04 - Minibatch Training",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()\n\n:::{.callout-warning} # How to install the module MINIAI pip install -e '.[dev]' :::"
  },
  {
    "objectID": "posts/course22p2/12_accel_sgd.html",
    "href": "posts/course22p2/12_accel_sgd.html",
    "title": "12 - Accelerated SGD",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nAccelerated SGD, Optimizers, Momentum, RMSProp, Schedulers, Scheduler callbacks, 1cycle training"
  },
  {
    "objectID": "posts/course22p2/12_accel_sgd.html#optimizers",
    "href": "posts/course22p2/12_accel_sgd.html#optimizers",
    "title": "12 - Accelerated SGD",
    "section": "Optimizers",
    "text": "Optimizers\n\nSGD\n\nclass SGD:\n    def __init__(self, params, lr, wd=0.):\n        params = list(params)\n        fc.store_attr()\n        self.i = 0\n\n    def step(self):\n        with torch.no_grad():\n            for p in self.params:\n                self.reg_step(p)\n                self.opt_step(p)\n        self.i +=1\n\n    def opt_step(self, p): p -= p.grad * self.lr\n    def reg_step(self, p):\n        if self.wd != 0: p *= 1 - self.lr*self.wd\n\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=0.4, cbs=cbs, opt_func=SGD)\n\n\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.773\n      0.641\n      0\n      train\n    \n    \n      0.825\n      0.485\n      0\n      eval\n    \n    \n      0.845\n      0.425\n      1\n      train\n    \n    \n      0.844\n      0.429\n      1\n      eval\n    \n    \n      0.863\n      0.376\n      2\n      train\n    \n    \n      0.852\n      0.406\n      2\n      eval\n    \n  \n\n\n\n\n\n\nConsider the difference between weight decay and L2 regularization:\nweight -= lr*wd*weight\n…vs…\nweight.grad += wd*weight\n\n\nMomentum\n\n\n\n\n\n\nVideo is great.\n\n\n\ncheck it lesson 17 1:41:03, Insight about batchsize is very interesting and beatiful.\n\n\n\nxs = torch.linspace(-4, 4, 100)\nys = 1 - (xs/3) ** 2 + torch.randn(100) * 0.1\n\n\n_,axs = plt.subplots(2,2, figsize=(12,8))\nbetas = [0.5,0.7,0.9,0.99]\nfor beta,ax in zip(betas, axs.flatten()):\n    ax.scatter(xs,ys)\n    avg,res = 0,[]\n    for yi in ys:\n        avg = beta*avg + (1-beta)*yi\n        res.append(avg)\n    ax.plot(xs,np.array(res), color='red');\n    ax.set_title(f'beta={beta}')\n\n\n\n\n\nclass Momentum(SGD):\n    def __init__(self, params, lr, wd=0., mom=0.9):\n        super().__init__(params, lr=lr, wd=wd)\n        self.mom=mom\n\n    def opt_step(self, p):\n        if not hasattr(p, 'grad_avg'): p.grad_avg = torch.zeros_like(p.grad)\n        p.grad_avg = p.grad_avg*self.mom + p.grad*(1-self.mom)\n        p -= self.lr * p.grad_avg\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=1.5, cbs=cbs, opt_func=Momentum)\n\n\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.784\n      0.597\n      0\n      train\n    \n    \n      0.845\n      0.423\n      0\n      eval\n    \n    \n      0.870\n      0.356\n      1\n      train\n    \n    \n      0.868\n      0.361\n      1\n      eval\n    \n    \n      0.886\n      0.311\n      2\n      train\n    \n    \n      0.876\n      0.343\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\n\nRMSProp\n\nclass RMSProp(SGD):\n    def __init__(self, params, lr, wd=0., sqr_mom=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.sqr_mom,self.eps = sqr_mom,eps\n\n    def opt_step(self, p):\n        if not hasattr(p, 'sqr_avg'): p.sqr_avg = p.grad**2\n        p.sqr_avg = p.sqr_avg*self.sqr_mom + p.grad**2*(1-self.sqr_mom)\n        p -= self.lr * p.grad/(p.sqr_avg.sqrt() + self.eps)\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=3e-3, cbs=cbs, opt_func=RMSProp)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.768\n      0.660\n      0\n      train\n    \n    \n      0.818\n      0.489\n      0\n      eval\n    \n    \n      0.847\n      0.417\n      1\n      train\n    \n    \n      0.844\n      0.430\n      1\n      eval\n    \n    \n      0.864\n      0.368\n      2\n      train\n    \n    \n      0.853\n      0.407\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\n\nAdam\n\nclass Adam(SGD):\n    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.beta1,self.beta2,self.eps = beta1,beta2,eps\n\n    def opt_step(self, p):\n        if not hasattr(p, 'avg'): p.avg = torch.zeros_like(p.grad.data)\n        if not hasattr(p, 'sqr_avg'): p.sqr_avg = torch.zeros_like(p.grad.data)\n        p.avg = self.beta1*p.avg + (1-self.beta1)*p.grad\n        unbias_avg = p.avg / (1 - (self.beta1**(self.i+1)))\n        p.sqr_avg = self.beta2*p.sqr_avg + (1-self.beta2)*(p.grad**2)\n        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i+1)))\n        p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=Adam)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.790\n      0.582\n      0\n      train\n    \n    \n      0.841\n      0.431\n      0\n      eval\n    \n    \n      0.867\n      0.363\n      1\n      train\n    \n    \n      0.863\n      0.376\n      1\n      eval\n    \n    \n      0.884\n      0.315\n      2\n      train\n    \n    \n      0.871\n      0.349\n      2\n      eval"
  },
  {
    "objectID": "posts/course22p2/12_accel_sgd.html#schedulers",
    "href": "posts/course22p2/12_accel_sgd.html#schedulers",
    "title": "12 - Accelerated SGD",
    "section": "Schedulers",
    "text": "Schedulers\nWe’ve already seen how we can easily write a custom LR-adjusting callback or Learner, or can use the predefined PyTorch schedulers. We’ll use the predefined ones for now since there’s nothing new to learn in implementing them ourselves.\n\n' '.join(o for o in dir(lr_scheduler) if o[0].isupper() and o[1].islower())\n\n'ChainedScheduler ConstantLR CosineAnnealingLR CosineAnnealingWarmRestarts Counter CyclicLR ExponentialLR LambdaLR LinearLR MultiStepLR MultiplicativeLR OneCycleLR Optimizer PolynomialLR ReduceLROnPlateau SequentialLR StepLR'\n\n\n\n' '.join(filter(lambda x: x[0].isupper() and x[1].islower(), dir(lr_scheduler)))\n\n'ChainedScheduler ConstantLR CosineAnnealingLR CosineAnnealingWarmRestarts Counter CyclicLR ExponentialLR LambdaLR LinearLR MultiStepLR MultiplicativeLR OneCycleLR Optimizer PolynomialLR ReduceLROnPlateau SequentialLR StepLR'\n\n\n\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, lr=6e-3, cbs=[DeviceCB(), SingleBatchCB()])\nlearn.fit(1)\n\n\nopt = learn.opt\n' '.join(o for o in dir(opt) if o[0]!='_')\n\n'add_param_group defaults load_state_dict param_groups state state_dict step zero_grad'\n\n\n\nopt\n\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.006\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n\n\n\nparam = next(iter(learn.model.parameters()))\nst = opt.state[param]\n\n\nst\n\n{'momentum_buffer': None}\n\n\n\nlen(opt.param_groups)\n\n1\n\n\n\npg = opt.param_groups[0]\n\n\nlist(pg)\n\n['params',\n 'lr',\n 'momentum',\n 'dampening',\n 'weight_decay',\n 'nesterov',\n 'maximize',\n 'foreach',\n 'differentiable']\n\n\n\nsched = lr_scheduler.CosineAnnealingLR(opt, 100)\n\n\nsched.base_lrs\n\n[0.006]\n\n\n\nsched.get_last_lr()\n\n[0.006]\n\n\n\ndef sched_lrs(sched, steps):\n    lrs = [sched.get_last_lr()]\n    for i in range(steps):\n        sched.optimizer.step()\n        sched.step()\n        lrs.append(sched.get_last_lr())\n    plt.plot(lrs)\n\n\nsched_lrs(sched, 110)\n\n\n\n\n\nScheduler callbacks\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass BaseSchedCB(Callback):\n    def __init__(self, sched): self.sched = sched\n    def before_fit(self, learn): self.schedo = self.sched(learn.opt)\n    def _step(self, learn):\n        if learn.training: self.schedo.step()\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass BatchSchedCB(BaseSchedCB):\n    def after_batch(self, learn): self._step(learn)\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass HasLearnCB(Callback):\n    def before_fit(self, learn): self.learn = learn \n    def after_fit(self, learn): self.learn = None\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass RecorderCB(Callback):\n    def __init__(self, **d): self.d = d\n    def before_fit(self, learn):\n        self.recs = {k:[] for k in self.d}\n        self.pg = learn.opt.param_groups[0]\n    \n    def after_batch(self, learn):\n        if not learn.training: return\n        for k,v in self.d.items():\n            self.recs[k].append(v(self))\n\n    def plot(self):\n        for k,v in self.recs.items():\n            plt.plot(v, label=k)\n            plt.legend()\n            plt.show()\n:::\n\ndef _lr(cb): return cb.pg['lr']\n\n\nlen(dls.train)\n\n59\n\n\n\ntmax = 3 * len(dls.train)\nsched = partial(lr_scheduler.CosineAnnealingLR, T_max=tmax)\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nrec = RecorderCB(lr=_lr)\nxtra = [BatchSchedCB(sched),rec]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=2e-2, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.809\n      0.515\n      0\n      train\n    \n    \n      0.858\n      0.383\n      0\n      eval\n    \n    \n      0.881\n      0.327\n      1\n      train\n    \n    \n      0.874\n      0.339\n      1\n      eval\n    \n    \n      0.898\n      0.280\n      2\n      train\n    \n    \n      0.883\n      0.317\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nrec.plot()\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass EpochSchedCB(BaseSchedCB):\n    def after_epoch(self, learn): self._step(learn)\n:::\n\nsched = partial(lr_scheduler.CosineAnnealingLR, T_max=3)\nset_seed(42)\nxtra = [EpochSchedCB(sched),rec]\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=2e-2, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(3)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.809\n      0.517\n      0\n      train\n    \n    \n      0.857\n      0.382\n      0\n      eval\n    \n    \n      0.881\n      0.327\n      1\n      train\n    \n    \n      0.875\n      0.339\n      1\n      eval\n    \n    \n      0.899\n      0.275\n      2\n      train\n    \n    \n      0.887\n      0.307\n      2\n      eval\n    \n  \n\n\n\n\n\n\n\nrec.plot()\n\n\n\n\n\n\n1cycle training\nPaper by Leslie Smith.\n\ndef _beta1(cb): return cb.pg['betas'][0]\nrec = RecorderCB(lr=_lr, mom=_beta1)\n\n\nset_seed(42)\nlr,epochs = 6e-2,5\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), rec]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.765\n      0.662\n      0\n      train\n    \n    \n      0.822\n      0.546\n      0\n      eval\n    \n    \n      0.862\n      0.376\n      1\n      train\n    \n    \n      0.856\n      0.413\n      1\n      eval\n    \n    \n      0.888\n      0.304\n      2\n      train\n    \n    \n      0.879\n      0.333\n      2\n      eval\n    \n    \n      0.904\n      0.257\n      3\n      train\n    \n    \n      0.901\n      0.279\n      3\n      eval\n    \n    \n      0.924\n      0.210\n      4\n      train\n    \n    \n      0.906\n      0.267\n      4\n      eval\n    \n  \n\n\n\n\n\n\n\nrec.plot()"
  },
  {
    "objectID": "posts/course22p2/12_accel_sgd.html#export--",
    "href": "posts/course22p2/12_accel_sgd.html#export--",
    "title": "12 - Accelerated SGD",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "posts/course22p2/09_learner.html",
    "href": "posts/course22p2/09_learner.html",
    "title": "09 - Learner",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nLearner, the most important so far! First experiment everything and second understand decorators. (real python stuff). keys: learning rate scheduler,learning rate finder, callback, metrics\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#learner",
    "href": "posts/course22p2/09_learner.html#learner",
    "title": "09 - Learner",
    "section": "Learner",
    "text": "Learner\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n\n\n\n\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n\nbs = 1024\ntds = dsd.with_transform(transformi)\n\n\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 784]), tensor([5, 4, 9, 4, 3, 0, 6, 5, 7, 6]))\n\n\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.xb,self.yb = to_device(self.batch)\n        self.preds = self.model(self.xb)\n        self.loss = self.loss_func(self.preds, self.yb)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n        with torch.no_grad(): self.calc_stats()\n\n    def calc_stats(self):\n        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n        self.accs.append(acc)\n        n = len(self.xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n\n    def one_epoch(self, train):\n        self.model.training = train\n        dl = self.dls.train if train else self.dls.valid\n        for self.num,self.batch in enumerate(dl): self.one_batch()\n        n = sum(self.ns)\n        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n    \n    def fit(self, n_epochs):\n        self.accs,self.losses,self.ns = [],[],[]\n        self.model.to(def_device)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.n_epochs = n_epochs\n        for self.epoch in range(n_epochs):\n            self.one_epoch(True)\n            with torch.no_grad(): self.one_epoch(False)\n\n\nm,nh = 28*28,50\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2)\nlearn.fit(1)\n\n0 True 1.1753063802083332 0.5986166666666667\n0 False 1.1203783482142857 0.6133857142857143"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#basic-callbacks-learner",
    "href": "posts/course22p2/09_learner.html#basic-callbacks-learner",
    "title": "09 - Learner",
    "section": "Basic Callbacks Learner",
    "text": "Basic Callbacks Learner\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass CancelFitException(Exception): pass\nclass CancelBatchException(Exception): pass\nclass CancelEpochException(Exception): pass\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass Callback(): order = 0\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef run_cbs(cbs, method_nm, learn=None):\n    for cb in sorted(cbs, key=attrgetter('order')):\n        method = getattr(cb, method_nm, None)\n        if method is not None: method(learn)\n:::\n\nclass CompletionCB(Callback):\n    def before_fit(self, learn): self.count = 0\n    def after_batch(self, learn): self.count += 1\n    def after_fit(self, learn): print(f'Completed {self.count} batches')\n\n\ncbs = [CompletionCB()]\nrun_cbs(cbs, 'before_fit')\nrun_cbs(cbs, 'after_batch')\nrun_cbs(cbs, 'after_fit')\n\nCompleted 1 batches\n\n\n\nclass Learner():\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.preds = self.model(self.batch[0])\n        self.loss = self.loss_func(self.preds, self.batch[1])\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n\n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        try:\n            self.callback('before_epoch')\n            for self.iter,self.batch in enumerate(self.dl):\n                try:\n                    self.callback('before_batch')\n                    self.one_batch()\n                    self.callback('after_batch')\n                except CancelBatchException: pass\n            self.callback('after_epoch')\n        except CancelEpochException: pass\n    \n    def fit(self, n_epochs):\n        self.n_epochs = n_epochs\n        self.epochs = range(n_epochs)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        try:\n            self.callback('before_fit')\n            for self.epoch in self.epochs:\n                self.one_epoch(True)\n                self.one_epoch(False)\n            self.callback('after_fit')\n        except CancelFitException: pass\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n\n\nm,nh = 28*28,50\ndef get_model(): return nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nmodel = get_model()\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[CompletionCB()])\nlearn.fit(1)\n\nCompleted 64 batches\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass SingleBatchCB(Callback):\n    order = 1\n    def after_batch(self, learn): raise CancelFitException()\n:::\n\nlearn = Learner(get_model(), dls, F.cross_entropy, lr=0.2, cbs=[SingleBatchCB(), CompletionCB()])\nlearn.fit(1)"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#metrics",
    "href": "posts/course22p2/09_learner.html#metrics",
    "title": "09 - Learner",
    "section": "Metrics",
    "text": "Metrics\n\nclass Metric:\n    def __init__(self): self.reset()\n    def reset(self): self.vals,self.ns = [],[]\n    def add(self, inp, targ=None, n=1):\n        self.last = self.calc(inp, targ)\n        self.vals.append(self.last)\n        self.ns.append(n)\n    @property\n    def value(self):\n        ns = tensor(self.ns)\n        return (tensor(self.vals)*ns).sum()/ns.sum()\n    def calc(self, inps, targs): return inps\n\n\nclass Accuracy(Metric):\n    def calc(self, inps, targs): return (inps==targs).float().mean()\n\n\nacc = Accuracy()\nacc.add(tensor([0, 1, 2, 0, 1, 2]), tensor([0, 1, 1, 2, 1, 0]))\nacc.add(tensor([1, 1, 2, 0, 1]), tensor([0, 1, 1, 2, 1]))\nacc.value\n\ntensor(0.45)\n\n\n\nloss = Metric()\nloss.add(0.6, n=32)\nloss.add(0.9, n=2)\nloss.value, round((0.6*32+0.9*2)/(32+2), 2)\n\n(tensor(0.62), 0.62)"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#some-callbacks",
    "href": "posts/course22p2/09_learner.html#some-callbacks",
    "title": "09 - Learner",
    "section": "Some callbacks",
    "text": "Some callbacks\n\n\n\n\n\n\ntorcheval\n\n\n\npip install torcheval check it if it is possible to use with the my project.\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n:::\n\nmetric = MulticlassAccuracy()\nmetric.update(tensor([0, 2, 1, 3]), tensor([0, 1, 2, 3]))\nmetric.compute()\n\ntensor(0.50)\n\n\n\nmetric.reset()\nmetric.compute()\n\ntensor(nan)\n\n\n\n\n\n\n\n\npossible GPU bug.\n\n\n\nJeremy says there may be a bug while using gpu for the metrics calculation thats the reason of to_cpu function exist.\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef to_cpu(x):\n    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n    if isinstance(x, list): return [to_cpu(o) for o in x]\n    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n    return x.detach().cpu()\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): print(d)\n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n\n    def after_batch(self, learn):\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass DeviceCB(Callback):\n    def __init__(self, device=def_device): fc.store_attr()\n    def before_fit(self, learn):\n        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)\n:::\n\nmodel = get_model()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[DeviceCB(), metrics])\nlearn.fit(1)\n\n{'accuracy': '0.602', 'loss': '1.183', 'epoch': 0, 'train': 'train'}\n{'accuracy': '0.700', 'loss': '0.847', 'epoch': 0, 'train': 'eval'}"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#flexible-learner",
    "href": "posts/course22p2/09_learner.html#flexible-learner",
    "title": "09 - Learner",
    "section": "Flexible learner",
    "text": "Flexible learner\n\n\n\n\n\n\ncontextmanager removed! Below learner is not the last version\n\n\n\nit is good to understand contextmanager but the code is changed\n\n\n\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @contextmanager\n    def cb_ctx(self, nm):\n        try:\n            self.callback(f'before_{nm}')\n            yield\n            self.callback(f'after_{nm}')\n        except globals()[f'Cancel{nm.title()}Exception']: pass\n        finally: self.callback(f'cleanup_{nm}')\n                \n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        with self.cb_ctx('epoch'):\n            for self.iter,self.batch in enumerate(self.dl):\n                with self.cb_ctx('batch'):\n                    self.predict()\n                    self.get_loss()\n                    if self.training:\n                        self.backward()\n                        self.step()\n                        self.zero_grad()\n    \n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n            with self.cb_ctx('fit'):\n                for self.epoch in self.epochs:\n                    if train: self.one_epoch(True)\n                    if valid: torch.no_grad()(self.one_epoch)(False)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass TrainCB(Callback):\n    def __init__(self, n_inp=1): self.n_inp = n_inp\n    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n    def backward(self, learn): learn.loss.backward()\n    def step(self, learn): learn.opt.step()\n    def zero_grad(self, learn): learn.opt.zero_grad()\n:::\nNB: I added self.n_inp after the lesson. This allows us to train models with more than one input or output.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass ProgressCB(Callback):\n    order = MetricsCB.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n        self.losses = []\n\n    def _log(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.training:\n            self.losses.append(learn.loss.item())\n            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])\n:::\n\nmodel = get_model()\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.596\n      1.167\n      0\n      train\n    \n    \n      0.729\n      0.794\n      0\n      eval"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#updated-versions-since-the-lesson",
    "href": "posts/course22p2/09_learner.html#updated-versions-since-the-lesson",
    "title": "09 - Learner",
    "section": "Updated versions since the lesson",
    "text": "Updated versions since the lesson\nAfter the lesson we noticed that contextlib.context_manager has a surprising “feature” which doesn’t let us raise an exception before the yield. Therefore we’ve replaced the context manager with a decorator in this updated version of Learner. We have also added a few more callbacks in one_epoch().\n\n\n\n\n\n\nwith_cbs\n\n\n\nthe explanation for this is in Lesson 15 around 1:28, o in the code is I think is self that means Learner Class itself.\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass with_cbs:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            #print(o)\n            try:\n                o.callback(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.nm}')\n            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.nm}')\n        return _f\n:::\n\n\n\n\n\n\nUse this learner instead.\n\n\n\nThis is the one.\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            self.backward()\n            self.callback('after_backward')\n            self.step()\n            self.callback('after_step')\n            self.zero_grad()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            if lr is None: lr = self.lr\n            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n            self._fit(train, valid)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training\n:::\n\nmodel = get_model()\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n<__main__.Learner object>\n\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.616\n      1.168\n      0\n      train\n    \n    \n      0.719\n      0.789\n      0\n      eval\n    \n  \n\n\n\n<__main__.Learner object>\n<__main__.Learner object>\n\n\n\n\n\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>\n<__main__.Learner object>"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#trainlearner-and-momentumlearner",
    "href": "posts/course22p2/09_learner.html#trainlearner-and-momentumlearner",
    "title": "09 - Learner",
    "section": "TrainLearner and MomentumLearner",
    "text": "TrainLearner and MomentumLearner\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n:::\n\n# NB: No TrainCB\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.676\n      0.936\n      0\n      train\n    \n    \n      0.791\n      0.585\n      0\n      eval"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#lrfindercb",
    "href": "posts/course22p2/09_learner.html#lrfindercb",
    "title": "09 - Learner",
    "section": "LRFinderCB",
    "text": "LRFinderCB\n\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3): fc.store_attr()\n    \n    def before_fit(self, learn):\n        self.lrs,self.losses = [],[]\n        self.min = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        self.losses.append(loss)\n        if loss < self.min: self.min = loss\n        if loss > self.min*3: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult\n\n\nlrfind = LRFinderCB()\ncbs = [DeviceCB(), lrfind]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-4, cbs=cbs)\nlearn.fit(1)\nplt.plot(lrfind.lrs, lrfind.losses)\nplt.xscale('log')\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom torch.optim.lr_scheduler import ExponentialLR\n:::\nExponentialLR\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass LRFinderCB(Callback):\n    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()\n    \n    def before_fit(self, learn):\n        self.sched = ExponentialLR(learn.opt, self.gamma)\n        self.lrs,self.losses = [],[]\n        self.min = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        self.losses.append(loss)\n        if loss < self.min: self.min = loss\n        if math.isnan(loss) or (loss > self.min*self.max_mult):\n            raise CancelFitException()\n        self.sched.step()\n\n    def cleanup_fit(self, learn):\n        plt.plot(self.lrs, self.losses)\n        plt.xscale('log')\n:::\n\ncbs = [DeviceCB()]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-5, cbs=cbs)\nlearn.fit(3, cbs=LRFinderCB())\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n@fc.patch\ndef lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):\n    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))\n:::\nlr_find was added in lesson 18. It’s just a shorter way of using LRFinderCB.\n\nMomentumLearner(get_model(), dls, F.cross_entropy, cbs=cbs).lr_find()"
  },
  {
    "objectID": "posts/course22p2/09_learner.html#export--",
    "href": "posts/course22p2/09_learner.html#export--",
    "title": "09 - Learner",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "posts/course22p2/06_foundations.html",
    "href": "posts/course22p2/06_foundations.html",
    "title": "06 - Callbacks",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nCallbacks and dunder methods"
  },
  {
    "objectID": "posts/course22p2/06_foundations.html#callbacks",
    "href": "posts/course22p2/06_foundations.html#callbacks",
    "title": "06 - Callbacks",
    "section": "Callbacks",
    "text": "Callbacks\n\nCallbacks as GUI events\n\nimport ipywidgets as widgets\n\nFrom the ipywidget docs:\n\nthe button widget is used to handle mouse clicks. The on_click method of the Button can be used to register function to be called when the button is clicked\n\n\nw = widgets.Button(description='Click me')\n\n\nw\n\n\n\n\n\ndef f(o): print('hi')\n\n\nw.on_click(f)\n\nNB: When callbacks are used in this way they are often called “events”.\n\n\nCreating your own callback\n\nfrom time import sleep\n\n\ndef slow_calculation():\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1)\n    return res\n\n\nslow_calculation()\n\n30\n\n\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1)\n        if cb: cb(i)\n    return res\n\n\ndef show_progress(epoch): print(f\"Awesome! We've finished epoch {epoch}!\")\n\n\nslow_calculation(show_progress)\n\nAwesome! We've finished epoch 0!\nAwesome! We've finished epoch 1!\nAwesome! We've finished epoch 2!\nAwesome! We've finished epoch 3!\nAwesome! We've finished epoch 4!\n\n\n30\n\n\n\n\nLambdas and partials\n\nslow_calculation(lambda o: print(f\"Awesome! We've finished epoch {o}!\"))\n\nAwesome! We've finished epoch 0!\nAwesome! We've finished epoch 1!\nAwesome! We've finished epoch 2!\nAwesome! We've finished epoch 3!\nAwesome! We've finished epoch 4!\n\n\n30\n\n\n\ndef show_progress(exclamation, epoch): print(f\"{exclamation}! We've finished epoch {epoch}!\")\n\n\nslow_calculation(lambda o: show_progress(\"OK I guess\", o))\n\nOK I guess! We've finished epoch 0!\nOK I guess! We've finished epoch 1!\nOK I guess! We've finished epoch 2!\nOK I guess! We've finished epoch 3!\nOK I guess! We've finished epoch 4!\n\n\n30\n\n\n\ndef make_show_progress(exclamation):\n    def _inner(epoch): print(f\"{exclamation}! We've finished epoch {epoch}!\")\n    return _inner\n\n\nslow_calculation(make_show_progress(\"Nice!\"))\n\nNice!! We've finished epoch 0!\nNice!! We've finished epoch 1!\nNice!! We've finished epoch 2!\nNice!! We've finished epoch 3!\nNice!! We've finished epoch 4!\n\n\n30\n\n\n\nfrom functools import partial\n\n\nslow_calculation(partial(show_progress, \"OK I guess\"))\n\nOK I guess! We've finished epoch 0!\nOK I guess! We've finished epoch 1!\nOK I guess! We've finished epoch 2!\nOK I guess! We've finished epoch 3!\nOK I guess! We've finished epoch 4!\n\n\n30\n\n\n\nf2 = partial(show_progress, \"OK I guess\")\n\n\n\nCallbacks as callable classes\n\nclass ProgressShowingCallback():\n    def __init__(self, exclamation=\"Awesome\"): self.exclamation = exclamation\n    def __call__(self, epoch): print(f\"{self.exclamation}! We've finished epoch {epoch}!\")\n\n\ncb = ProgressShowingCallback(\"Just super\")\n\n\nslow_calculation(cb)\n\nJust super! We've finished epoch 0!\nJust super! We've finished epoch 1!\nJust super! We've finished epoch 2!\nJust super! We've finished epoch 3!\nJust super! We've finished epoch 4!\n\n\n30\n\n\n\n\nMultiple callback funcs; *args and **kwargs\n\ndef f(*a, **b): print(f\"args: {a}; kwargs: {b}\")\n\n\nf(3, 'a', thing1=\"hello\")\n\nargs: (3, 'a'); kwargs: {'thing1': 'hello'}\n\n\n\ndef g(a,b,c=0): print(a,b,c)\n\n\nargs = [1,2]\nkwargs = {'c':3}\ng(*args, **kwargs)\n\n1 2 3\n\n\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb: cb.before_calc(i)\n        res += i*i\n        sleep(1)\n        if cb: cb.after_calc(i, val=res)\n    return res\n\n\nclass PrintStepCallback():\n    def before_calc(self, *args, **kwargs): print(f\"About to start\")\n    def after_calc (self, *args, **kwargs): print(f\"Done step\")\n\n\nslow_calculation(PrintStepCallback())\n\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\n\n\n30\n\n\n\nclass PrintStatusCallback():\n    def __init__(self): pass\n    def before_calc(self, epoch, **kwargs): print(f\"About to start: {epoch}\")\n    def after_calc (self, epoch, val, **kwargs): print(f\"After {epoch}: {val}\")\n\n\nslow_calculation(PrintStatusCallback())\n\nAbout to start: 0\nAfter 0: 0\nAbout to start: 1\nAfter 1: 1\nAbout to start: 2\nAfter 2: 5\nAbout to start: 3\nAfter 3: 14\nAbout to start: 4\nAfter 4: 30\n\n\n30\n\n\n\n\nModifying behavior\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb and hasattr(cb,'before_calc'): cb.before_calc(i)\n        res += i*i\n        sleep(1)\n        if cb and hasattr(cb,'after_calc'):\n            if cb.after_calc(i, res):\n                print(\"stopping early\")\n                break\n    return res\n\n\nclass PrintAfterCallback():\n    def after_calc (self, epoch, val):\n        print(f\"After {epoch}: {val}\")\n        if val>10: return True\n\n\nslow_calculation(PrintAfterCallback())\n\nAfter 0: 0\nAfter 1: 1\nAfter 2: 5\nAfter 3: 14\nstopping early\n\n\n14\n\n\n\nclass SlowCalculator():\n    def __init__(self, cb=None): self.cb,self.res = cb,0\n    \n    def callback(self, cb_name, *args):\n        if not self.cb: return\n        cb = getattr(self.cb,cb_name, None)\n        if cb: return cb(self, *args)\n\n    def calc(self):\n        for i in range(5):\n            self.callback('before_calc', i)\n            self.res += i*i\n            sleep(1)\n            if self.callback('after_calc', i):\n                print(\"stopping early\")\n                break\n\n\nclass ModifyingCallback():\n    def after_calc (self, calc, epoch):\n        print(f\"After {epoch}: {calc.res}\")\n        if calc.res>10: return True\n        if calc.res<3: calc.res = calc.res*2\n\n\ncalculator = SlowCalculator(ModifyingCallback())\n\n\ncalculator.calc()\ncalculator.res\n\nAfter 0: 0\nAfter 1: 1\nAfter 2: 6\nAfter 3: 15\nstopping early\n\n\n15"
  },
  {
    "objectID": "posts/course22p2/06_foundations.html#dunder__-thingies",
    "href": "posts/course22p2/06_foundations.html#dunder__-thingies",
    "title": "06 - Callbacks",
    "section": "__dunder__ thingies",
    "text": "__dunder__ thingies\nAnything that looks like __this__ is, in some way, special. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call __init__. These are defined as part of the python data model.\nFor instance, if python sees +, then it will call the special method __add__. If you try to display an object in Jupyter (or lots of other places in Python) it will call __repr__.\n\nclass SloppyAdder():\n    def __init__(self,o): self.o=o\n    def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01)\n    def __repr__(self): return str(self.o)\n\n\na = SloppyAdder(1)\nb = SloppyAdder(2)\na+b\n\n3.01\n\n\nSpecial methods you should probably know about (see data model link above) are:\n\n__getitem__\n__getattr__\n__setattr__\n__del__\n__init__\n__new__\n__enter__\n__exit__\n__len__\n__repr__\n__str__\n\n\n__getattr__ and getattr\n\nclass A: a,b=1,2\n\n\na = A()\n\n\na.b\n\n2\n\n\n\ngetattr(a, 'b')\n\n2\n\n\n\ngetattr(a, 'b' if random.random()>0.5 else 'a')\n\n2\n\n\n\nclass B:\n    a,b=1,2\n    def __getattr__(self, k):\n        if k[0]=='_': raise AttributeError(k)\n        return f'Hello from {k}'\n\n\nb = B()\n\n\nb.a\n\n1\n\n\n\nb.foo\n\n'Hello from foo'"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html",
    "href": "posts/course22p2/05_datasets.html",
    "title": "05 - Datasets",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nDatasets, Dataloaders\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#hugging-face-datasets",
    "href": "posts/course22p2/05_datasets.html#hugging-face-datasets",
    "title": "05 - Datasets",
    "section": "Hugging Face Datasets",
    "text": "Hugging Face Datasets\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nprint(ds_builder.info.description)\n\n\nds_builder.info.features\n\n\nds_builder.info.splits\n\n\ndsd = load_dataset(name)\ndsd\n\n\ntrain,test = dsd['train'],dsd['test']\ntrain[0]\n\n\nx,y = ds_builder.info.features\n\n\nx,y\n\n\nx,y = 'image','label'\nimg = train[0][x]\nimg\n\n\nxb = train[:5][x]\nyb = train[:5][y]\nyb\n\n\nfeaty = train.features[y]\nfeaty\n\n\nfeaty.int2str(yb)\n\n\ntrain['label'][:5]\n\n\ndef collate_fn(b):\n    return {x:torch.stack([TF.to_tensor(o[x]) for o in b]),\n            y:tensor([o[y] for o in b])}\n\n\ndl = DataLoader(train, collate_fn=collate_fn, batch_size=16)\nb = next(iter(dl))\nb[x].shape,b[y]\n\n\ndef transforms(b):\n    b[x] = [TF.to_tensor(o) for o in b[x]]\n    return b\n\n\ntds = train.with_transform(transforms)\ndl = DataLoader(tds, batch_size=16)\nb = next(iter(dl))\nb[x].shape,b[y]\n\n\ndef _transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef inplace(f):\n    def _f(b):\n        f(b)\n        return b\n    return _f\n:::\n\ntransformi = inplace(_transformi)\n\n\nr = train.with_transform(transformi)[0]\nr[x].shape,r[y]\n\n\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n\ntdsf = train.with_transform(transformi)\nr = tdsf[0]\nr[x].shape,r[y]\n\n\nd = dict(a=1,b=2,c=3)\nig = itemgetter('a','c')\nig(d)\n\n\nclass D:\n    def __getitem__(self, k): return 1 if k=='a' else 2 if k=='b' else 3\n\n\nd = D()\nig(d)\n\n\nlist(tdsf.features)\n\n\nbatch = dict(a=[1],b=[2]), dict(a=[3],b=[4])\ndefault_collate(batch)"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#miniai",
    "href": "posts/course22p2/05_datasets.html#miniai",
    "title": "05 - Datasets",
    "section": "miniai",
    "text": "miniai\naround 1:17 there is an explanation of miniai and its installation"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#plotting-images",
    "href": "posts/course22p2/05_datasets.html#plotting-images",
    "title": "05 - Datasets",
    "section": "Plotting images",
    "text": "Plotting images\n\nb = next(iter(dl))\nxb = b['image']\nimg = xb[0]\nplt.imshow(img[0]);"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#kwargs",
    "href": "posts/course22p2/05_datasets.html#kwargs",
    "title": "05 - Datasets",
    "section": "**kwargs",
    "text": "**kwargs\n@fc.delegates makes imshow kwargs visible. Great."
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#export--",
    "href": "posts/course22p2/05_datasets.html#export--",
    "title": "05 - Datasets",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "posts/course22p2/10_activations.html",
    "href": "posts/course22p2/10_activations.html",
    "title": "10 - Activations",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nActivations, hooks and histograms."
  },
  {
    "objectID": "posts/course22p2/10_activations.html#baseline",
    "href": "posts/course22p2/10_activations.html#baseline",
    "title": "10 - Activations",
    "section": "Baseline",
    "text": "Baseline\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\ndef cnn_layers():\n    return [\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        nn.Flatten()]\n\nWe want to train quickly, so that means training at a high learning rate.\n\nfrom torcheval.metrics import MulticlassAccuracy\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n\n\ndef fit(model, epochs=1, xtra_cbs=None):\n    learn = Learner(model, dls, loss_func=F.cross_entropy, lr=0.6, cbs=cbs+fc.L(xtra_cbs))\n    learn.fit(epochs)\n    return learn\n\n\nset_seed(1)\nlearn = fit(nn.Sequential(*cnn_layers()))\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.208\n      2.243\n      0\n      train\n    \n    \n      0.204\n      2.165\n      0\n      eval"
  },
  {
    "objectID": "posts/course22p2/10_activations.html#hooks",
    "href": "posts/course22p2/10_activations.html#hooks",
    "title": "10 - Activations",
    "section": "Hooks",
    "text": "Hooks\n\nManual insertion\n\nclass SequentialModel(nn.Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        self.act_means = [[] for _ in layers]\n        self.act_stds  = [[] for _ in layers]\n        \n    def __call__(self, x):\n        for i,l in enumerate(self.layers):\n            x = l(x)\n            self.act_means[i].append(to_cpu(x).mean())\n            self.act_stds [i].append(to_cpu(x).std ())\n        return x\n    \n    def __iter__(self): return iter(self.layers)\n\n\nset_seed(1)\nmodel = SequentialModel(*cnn_layers())\nlearn = fit(model)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.247\n      2.110\n      0\n      train\n    \n    \n      0.376\n      1.659\n      0\n      eval\n    \n  \n\n\n\n\n\n\n\nfor l in model.act_means: plt.plot(l)\nplt.legend(range(5));\n\n\n\n\n\nfor l in model.act_stds: plt.plot(l)\nplt.legend(range(5));\n\n\n\n\n\n\nPytorch hooks\nHooks are PyTorch object you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook). Hooks don’t require us to rewrite the model.\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\nA hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list.\n\nact_means = [[] for _ in model]\nact_stds  = [[] for _ in model]\n\n\ndef append_stats(i, mod, inp, outp):\n    act_means[i].append(to_cpu(outp).mean())\n    act_stds [i].append(to_cpu(outp).std())\n\n\nfor i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i))\n\n\nfit(model)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.263\n      2.091\n      0\n      train\n    \n    \n      0.164\n      2.245\n      0\n      eval\n    \n  \n\n\n\n\n\n\n<miniai.learner.Learner>\n\n\n\nfor o in act_means: plt.plot(o)\nplt.legend(range(5));\n\n\n\n\n\n\nHook class\nWe can refactor this in a Hook class. It’s very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won’t be properly released when your model is deleted.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass Hook():\n    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n    def remove(self): self.hook.remove()\n    def __del__(self): self.remove()\n:::\n\ndef append_stats(hook, mod, inp, outp):\n    if not hasattr(hook,'stats'): hook.stats = ([],[])\n    acts = to_cpu(outp)\n    hook.stats[0].append(acts.mean())\n    hook.stats[1].append(acts.std())\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nhooks = [Hook(l, append_stats) for l in model[:5].children()]\n\n\nlearn = fit(model)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.247\n      2.122\n      0\n      train\n    \n    \n      0.397\n      1.456\n      0\n      eval\n    \n  \n\n\n\n\n\n\n\nfor h in hooks:\n    plt.plot(h.stats[0])\n    h.remove()\nplt.legend(range(5));\n\n\n\n\n\n\nA Hooks class\n\nclass DummyCtxMgr:\n    def __enter__(self, *args):\n        print(\"let's go!\")\n        return self\n    def __exit__ (self, *args): print(\"all done!\")\n    def hello(self): print(\"hello.\")\n\n\nwith DummyCtxMgr() as dcm: dcm.hello()\n\nlet's go!\nhello.\nall done!\n\n\n\nclass DummyList(list):\n    def __delitem__(self, i):\n        print(f\"Say bye to item {i}\")\n        super().__delitem__(i)\n\n\ndml = DummyList([1,3,2])\ndml\n\n[1, 3, 2]\n\n\n\ndel(dml[2])\ndml\n\nSay bye to item 2\n\n\n[1, 3]\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass Hooks(list):\n    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])\n    def __enter__(self, *args): return self\n    def __exit__ (self, *args): self.remove()\n    def __del__(self): self.remove()\n    def __delitem__(self, i):\n        self[i].remove()\n        super().__delitem__(i)\n    def remove(self):\n        for h in self: h.remove()\n:::\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nwith Hooks(model, append_stats) as hooks:\n    fit(model)\n    fig,axs = plt.subplots(1,2, figsize=(10,4))\n    for h in hooks:\n        for i in 0,1: axs[i].plot(h.stats[i])\n    plt.legend(range(6));\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.166\n      2.475\n      0\n      train\n    \n    \n      0.100\n      2.303\n      0\n      eval\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nHooksCallback\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass HooksCallback(Callback):\n    def __init__(self, hookfunc, mod_filter=fc.noop, on_train=True, on_valid=False, mods=None):\n        fc.store_attr()\n        super().__init__()\n    \n    def before_fit(self, learn):\n        if self.mods: mods=self.mods\n        else: mods = fc.filter_ex(learn.model.modules(), self.mod_filter)\n        self.hooks = Hooks(mods, partial(self._hookfunc, learn))\n\n    def _hookfunc(self, learn, *args, **kwargs):\n        if (self.on_train and learn.training) or (self.on_valid and not learn.training): self.hookfunc(*args, **kwargs)\n\n    def after_fit(self, learn): self.hooks.remove()\n    def __iter__(self): return iter(self.hooks)\n    def __len__(self): return len(self.hooks)\n:::\n\nhc = HooksCallback(append_stats, mod_filter=fc.risinstance(nn.Conv2d))\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtra_cbs=[hc]);\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.205\n      2.232\n      0\n      train\n    \n    \n      0.100\n      2.305\n      0\n      eval\n    \n  \n\n\n\n\n\n\n\nfig,axs = plt.subplots(1,2, figsize=(10,4))\nfor h in hc:\n    for i in 0,1: axs[i].plot(h.stats[i])\nplt.legend(range(6));"
  },
  {
    "objectID": "posts/course22p2/10_activations.html#histograms",
    "href": "posts/course22p2/10_activations.html#histograms",
    "title": "10 - Activations",
    "section": "Histograms",
    "text": "Histograms\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef append_stats(hook, mod, inp, outp):\n    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n    acts = to_cpu(outp)\n    hook.stats[0].append(acts.mean())\n    hook.stats[1].append(acts.std())\n    hook.stats[2].append(acts.abs().histc(40,0,10))\n:::\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\nhc = HooksCallback(append_stats, mod_filter=fc.risinstance(nn.Conv2d))\nfit(model, xtra_cbs=[hc]);\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.213\n      2.379\n      0\n      train\n    \n    \n      0.100\n      21.771\n      0\n      eval\n    \n  \n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n# Thanks to @ste for initial version of histgram plotting code\ndef get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()\n:::\n\nfig,axes = get_grid(len(hc), figsize=(11,5))\nfor ax,h in zip(axes.flat, hc):\n    show_image(get_hist(h), ax, origin='lower')\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef get_min(h):\n    h1 = torch.stack(h.stats[2]).t().float()\n    return h1[0]/h1.sum(0)\n:::\n\nfig,axes = get_grid(len(hc), figsize=(11,5))\nfor ax,h in zip(axes.flatten(), hc):\n    ax.plot(get_min(h))\n    ax.set_ylim(0,1)"
  },
  {
    "objectID": "posts/course22p2/10_activations.html#activationstats",
    "href": "posts/course22p2/10_activations.html#activationstats",
    "title": "10 - Activations",
    "section": "ActivationStats",
    "text": "ActivationStats\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass ActivationStats(HooksCallback):\n    def __init__(self, mod_filter=fc.noop): super().__init__(append_stats, mod_filter)\n\n    def color_dim(self, figsize=(11,5)):\n        fig,axes = get_grid(len(self), figsize=figsize)\n        for ax,h in zip(axes.flat, self):\n            show_image(get_hist(h), ax, origin='lower')\n\n    def dead_chart(self, figsize=(11,5)):\n        fig,axes = get_grid(len(self), figsize=figsize)\n        for ax,h in zip(axes.flatten(), self):\n            ax.plot(get_min(h))\n            ax.set_ylim(0,1)\n\n    def plot_stats(self, figsize=(10,4)):\n        fig,axs = plt.subplots(1,2, figsize=figsize)\n        for h in self:\n            for i in 0,1: axs[i].plot(h.stats[i])\n        axs[0].set_title('Means')\n        axs[1].set_title('Stdevs')\n        plt.legend(fc.L.range(self))\n:::\n\nastats = ActivationStats(fc.risinstance(nn.Conv2d))\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtra_cbs=[astats]);\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.208\n      2.199\n      0\n      train\n    \n    \n      0.289\n      1.695\n      0\n      eval\n    \n  \n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\n\nastats.plot_stats()"
  },
  {
    "objectID": "posts/course22p2/10_activations.html#export--",
    "href": "posts/course22p2/10_activations.html#export--",
    "title": "10 - Activations",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html",
    "href": "posts/course22p2/01_matmul.html",
    "title": "01 - Matrix multiplication from foundations",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page."
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#matrix-multiplication-from-foundations",
    "href": "posts/course22p2/01_matmul.html#matrix-multiplication-from-foundations",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Matrix multiplication from foundations",
    "text": "Matrix multiplication from foundations\nThe foundations we’ll assume throughout this course are:\n\nPython\nmatplotlib\nThe Python standard library\nJupyter notebooks and nbdev\n\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#get-data",
    "href": "posts/course22p2/01_matmul.html#get-data",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-rw-r-- 1 niyazi niyazi 17051982 Şub  5 11:17 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\n\n\n\n\n\nChoosing colormaps in Matplotlib\n\n\n\nhttps://matplotlib.org/stable/tutorials/colors/colormaps.html\n\n\n\n# this cahanges the colormap\nmpl.rcParams['image.cmap'] = 'magma'\n# mpl.pyplot == plt.imshow\nmpl.pyplot.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice>\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\n\n\n\n\n\nIter-callable usage\n\n\n\nif a callable passed to a iter(lambda) then it runs again and again until it thit the sentinel that is [] at this time\n\n\n\niter(lambda: list(islice(it, 28)), [])\n\n<callable_iterator>\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#matrix-and-tensor",
    "href": "posts/course22p2/01_matmul.html#matrix-and-tensor",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\n\nimgs[0,20,15]\n\ntensor(0.99)\n\n\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#random-numbers",
    "href": "posts/course22p2/01_matmul.html#random-numbers",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Random numbers",
    "text": "Random numbers\nQuantum random numbers from The Australian National University https://qrng.anu.edu.au/\nCloudFlare random number generator. https://blog.cloudflare.com/randomness-101-lavarand-in-production/\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.93])\nIn child: tensor([0.93])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n1.55 ms ± 8.41 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n%timeit -n 10 torch.randn(784,10)\n\n26.9 µs ± 13.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#matrix-multiplication",
    "href": "posts/course22p2/01_matmul.html#matrix-multiplication",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\n\n\n\n\n\n\nFor a better visibility change the linewidth from 80 to 140\n\n\n\n\n\n\n\ntorch.set_printoptions(precision=2, linewidth=80, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,\n          -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,\n          -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,\n          13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,\n           3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,\n           5.28]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\n\n\n\n\n\nThe same content with 140 characters\n\n\n\n\n\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\n\n%time _=matmul(m1, m2)\n\nCPU times: user 227 ms, sys: 0 ns, total: 227 ms\nWall time: 227 ms\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#numba",
    "href": "posts/course22p2/01_matmul.html#numba",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Numba",
    "text": "Numba\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 47.3 ms, sys: 3.69 ms, total: 50.9 ms\nWall time: 50.8 ms\n\n\n20.0\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 16 µs, sys: 0 ns, total: 16 µs\nWall time: 18.1 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n\n%timeit -n 50 matmul(m1a,m2a)\n\n148 µs ± 14.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#elementwise-ops",
    "href": "posts/course22p2/01_matmul.html#elementwise-ops",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nnk allways right to left no operator precedence\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\n\na + b\n\ntensor([12., 14.,  3.])\n\n\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nFrobenius norm:\n\\[\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}\\]\nHint: you don’t normally need to write equations in LaTeX yourself, instead, you can click ‘edit’ in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click “Download: Other formats” in the top right, then “Download source”; rename the downloaded file to end in .tgz if it doesn’t already, and you should find the source there, including the equations to copy and paste. This is the source LaTeX that I pasted to render the equation above:\n$$\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}$$\n\nsf = (m*m).sum()\nsf\n\ntensor(285.)\n\n\n\nsf.sqrt()\n\ntensor(16.88)\n\n\n\nm[2,:],m[:,2]\n\n(tensor([7., 8., 9.]), tensor([3., 6., 9.]))\n\n\n\nm[2]\n\ntensor([7., 8., 9.])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n279 µs ± 2.53 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = torch.dot(a[i,:], b[:,j])\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n236 µs ± 3.12 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#broadcasting",
    "href": "posts/course22p2/01_matmul.html#broadcasting",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Broadcasting",
    "text": "Broadcasting\nThe term broadcasting describes how arrays with different shapes are treated during arithmetic operations.\nFrom the Numpy Documentation:\nThe term broadcasting describes how numpy treats arrays with \ndifferent shapes during arithmetic operations. Subject to certain \nconstraints, the smaller array is “broadcast” across the larger \narray so that they have compatible shapes. Broadcasting provides a \nmeans of vectorizing array operations so that looping occurs in C\ninstead of Python. It does this without making needless copies of \ndata and usually leads to efficient algorithm implementations.\nIn addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\nThis section was adapted from Chapter 4 of the fast.ai Computational Linear Algebra course.\n\nBroadcasting with a scalar\n\na\n\ntensor([10.,  6., -4.])\n\n\n\na > 0\n\ntensor([ True,  True, False])\n\n\nHow are we able to do a > 0? 0 is being broadcast to have the same dimensions as a.\nFor instance you can normalize our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar), using broadcasting.\nOther examples of broadcasting with a scalar:\n\na + 1\n\ntensor([11.,  7., -3.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\n2*m\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\n\nBroadcasting a vector to a matrix\nAlthough broadcasting a scalar is an idea that dates back to APL, the more powerful idea of broadcasting across higher rank tensors comes from a little known language called Yorick.\nWe can also broadcast a vector to a matrix:\n\nc = tensor([10.,20,30]); c\n\ntensor([10., 20., 30.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nm.shape,c.shape\n\n(torch.Size([3, 3]), torch.Size([3]))\n\n\n\nm + c\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nc + m\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nt = c.expand_as(m)\n\n\nt\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\n\nm + t\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\nWe don’t really copy the rows, but it looks as if we did. In fact, the rows are given a stride of 0.\n\nt.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n\n\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\nYou can index with the special value [None] or use unsqueeze() to convert a 1-dimensional array into a 2-dimensional array (although one of those dimensions has value 1).\n\nc.shape\n\ntorch.Size([3])\n\n\n\nc.unsqueeze(0), c[None, :]\n\n(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]))\n\n\n\nc.shape, c.unsqueeze(0).shape\n\n(torch.Size([3]), torch.Size([1, 3]))\n\n\n\nc.unsqueeze(1), c[:, None]\n\n(tensor([[10.],\n         [20.],\n         [30.]]),\n tensor([[10.],\n         [20.],\n         [30.]]))\n\n\n\nc.shape, c.unsqueeze(1).shape\n\n(torch.Size([3]), torch.Size([3, 1]))\n\n\nYou can always skip trailling ‘:’s. And’…’ means ‘all preceding dimensions’\n\nc[None].shape,c[...,None].shape\n\n(torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\nc[:,None].expand_as(m)\n\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n\n\n\nm + c[:,None]\n\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n\n\n\nm + c[None,:]\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\n\nBroadcasting Rules\n\nc[None,:]\n\ntensor([[10., 20., 30.]])\n\n\n\nc[None,:].shape\n\ntorch.Size([1, 3])\n\n\n\nc[:,None]\n\ntensor([[10.],\n        [20.],\n        [30.]])\n\n\n\nc[:,None].shape\n\ntorch.Size([3, 1])\n\n\n\nc[None,:] * c[:,None]\n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\n\nc[None] > c[:,None]\n\ntensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])\n\n\n\nm*m\n\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n\n\nWhen operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when\n\nthey are equal, or\none of them is 1, in which case that dimension is broadcasted to make it the same size\n\nArrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\nImage  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\nThe numpy documentation includes several examples of what dimensions can and can not be broadcast together."
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#matmul-with-broadcasting",
    "href": "posts/course22p2/01_matmul.html#matmul-with-broadcasting",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\ndigit is the first digit in the dataset\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\nm2.shape\n\ntorch.Size([784, 10])\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n70.1 µs ± 1.97 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nOur time has gone from ~500ms to <0.1ms, an over 5000x improvement! We can run on the whole dataset now.\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 6.59 s, sys: 200 ms, total: 6.79 s\nWall time: 663 ms"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#einstein-summation",
    "href": "posts/course22p2/01_matmul.html#einstein-summation",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Einstein summation",
    "text": "Einstein summation\nEinstein summation (einsum) is a compact representation for combining products and sums in a general way. The key rules are:\n\nRepeating letters between input arrays means that values along those axes will be multiplied together.\nOmitting a letter from the output means that values along that axis will be summed.\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\n# c[i,j] += a[i,k] * b[k,j]\n# c[i,j] = (a[i,:] * b[:,j]).sum()\nmr = torch.einsum('ik,kj->ikj', m1, m2)\nmr.shape\n\ntorch.Size([5, 784, 10])\n\n\n\nmr.sum(1)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ntorch.einsum('ik,kj->ij', m1, m2)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n\n\ntest_close(tr, matmul(x_train, weights), eps=1e-3)\n\n\n%timeit -n 5 _=matmul(x_train, weights)\n\n15.1 ms ± 176 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#pytorch-op",
    "href": "posts/course22p2/01_matmul.html#pytorch-op",
    "title": "01 - Matrix multiplication from foundations",
    "section": "pytorch op",
    "text": "pytorch op\nWe can use pytorch’s function or operator directly for matrix multiplication.\n\ntest_close(tr, x_train@weights, eps=1e-3)\n\n\n%timeit -n 5 _=torch.matmul(x_train, weights)\n\n15.2 ms ± 96.2 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#cuda",
    "href": "posts/course22p2/01_matmul.html#cuda",
    "title": "01 - Matrix multiplication from foundations",
    "section": "CUDA",
    "text": "CUDA\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\nres = torch.zeros(ar, bc)\nmatmul((0,0), m1, m2, res)\nres\n\ntensor([[-10.94,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00]])\n\n\n\ndef launch_kernel(kernel, grid_x, grid_y, *args, **kwargs):\n    for i in range(grid_x):\n        for j in range(grid_y): kernel((i,j), *args, **kwargs)\n\n\nres = torch.zeros(ar, bc)\nlaunch_kernel(matmul, ar, bc, m1, m2, res)\nres\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nfrom numba import cuda\n\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\n@cuda.jit\ndef matmul(a,b,c):\n    i, j = cuda.grid(2)\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\nr = np.zeros(tr.shape)\nm1g,m2g,rg = map(cuda.to_device, (x_train,weights,r))\n\n\nr.shape\n\n(50000, 10)\n\n\n\nTPB = 16\nrr,rc = r.shape\nblockspergrid = (math.ceil(rr / TPB), math.ceil(rc / TPB))\nblockspergrid\n\n(3125, 1)\n\n\n\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\ntest_close(tr, r, eps=1e-3)\n\n\n%%timeit -n 10\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\n\n3.61 ms ± 708 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nm1c,m2c = x_train.cuda(),weights.cuda()\n\n\nr=(m1c@m2c).cpu()\n\n\n%timeit -n 10 r=(m1c@m2c).cpu()\n\n458 µs ± 93.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nOur broadcasting version was >500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!"
  },
  {
    "objectID": "posts/course22p2/03_backprop.html",
    "href": "posts/course22p2/03_backprop.html",
    "title": "03 - The forward and backward passes",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page."
  },
  {
    "objectID": "posts/course22p2/03_backprop.html#the-forward-and-backward-passes",
    "href": "posts/course22p2/03_backprop.html#the-forward-and-backward-passes",
    "title": "03 - The forward and backward passes",
    "section": "The forward and backward passes",
    "text": "The forward and backward passes\n\n\n\ncalculus.png\n\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nfrom pathlib import Path\nfrom torch import tensor\nfrom fastcore.test import test_close\ntorch.manual_seed(42)\n\nmpl.rcParams['image.cmap'] = 'gray'\ntorch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\nnp.set_printoptions(precision=2, linewidth=125)\n\npath_data = Path('data')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
  },
  {
    "objectID": "posts/course22p2/03_backprop.html#foundations-version",
    "href": "posts/course22p2/03_backprop.html#foundations-version",
    "title": "03 - The forward and backward passes",
    "section": "Foundations version",
    "text": "Foundations version\n\nBasic architecture\n\nn,m = x_train.shape\nc = y_train.max()+1\nn,m,c\n\n(50000, 784, tensor(10))\n\n\n\n# num hidden\nnh = 50\n\n\nw1 = torch.randn(m,nh)\nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)\nb2 = torch.zeros(1)\n\n\ndef lin(x, w, b): return x@w + b\n\n\nt = lin(x_valid, w1, b1)\nt.shape\n\ntorch.Size([10000, 50])\n\n\n\ndef relu(x): return x.clamp_min(0.)\n\n\nt = relu(t)\nt\n\ntensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n        ...,\n        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])\n\n\n\ndef model(xb):\n    l1 = lin(xb, w1, b1)\n    l2 = relu(l1)\n    return lin(l2, w2, b2)\n\n\nres = model(x_valid)\nres.shape\n\ntorch.Size([10000, 1])\n\n\n\n\nLoss function: MSE\n(Of course, mse is not a suitable loss function for multi-class classification; we’ll use a better loss function soon. We’ll use mse for now to keep things simple.)\n\nres.shape,y_valid.shape\n\n(torch.Size([10000, 1]), torch.Size([10000]))\n\n\n\n(res-y_valid).shape\n\ntorch.Size([10000, 10000])\n\n\nWe need to get rid of that trailing (,1), in order to use mse.\n\nres[:,0].shape\n\ntorch.Size([10000])\n\n\n\nres.squeeze().shape\n\ntorch.Size([10000])\n\n\n\n(res[:,0]-y_valid).shape\n\ntorch.Size([10000])\n\n\n\ny_train,y_valid = y_train.float(),y_valid.float()\n\npreds = model(x_train)\npreds.shape\n\ntorch.Size([50000, 1])\n\n\n\ndef mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n\n\nmse(preds, y_train)\n\ntensor(4308.76)\n\n\n\n\nGradients and backward pass\n\nfrom sympy import symbols,diff\nx,y = symbols('x y')\ndiff(x**2, x)\n\n\\(\\displaystyle 2 x\\)\n\n\n\ndiff(3*x**2+9, x)\n\n\\(\\displaystyle 6 x\\)\n\n\n\n\n\n\n\n\nSteps of the back-propagation\n\n\n\n\n\n\n\n\n\nchain.png\n\n\n\n\n\n\n\n\nFurther reading\n\n\n\n\ncheck here too for a more detailed explanation of backpropagation. Same thing but includes extra-steps. https://course19.fast.ai/videos/?lesson=8m around 1:53 code here too https://github.com/fastai/course-v3/blob/master/nbs/dl2/02_fully_connected.ipynb\n\nGradient of the relu.\nmse grad( gradient of the loss)\ngradient of the linear layer.\n\n\nThe Intuitive Notion of the Chain Rule\n\nhttps://webspace.ship.edu/msrenault/geogebracalculus/derivative_intuitive_chain_rule.html\n\nThe Matrix Calculus You Need For Deep Learning\n\nhttps://explained.ai/matrix-calculus/\n\n\n\n\n\n# keywords python debugger debug\n\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    #import pdb; pdb.set_trace()\n    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n    b.g = out.g.sum(0)\n\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = lin(inp, w1, b1)\n    l2 = relu(l1)\n    out = lin(l2, w2, b2)\n    diff = out[:,0]-targ\n    loss = diff.pow(2).mean()\n    \n    # backward pass:\n    out.g = 2.*diff[:,None] / inp.shape[0]\n    lin_grad(l2, out, w2, b2)\n    l1.g = (l1>0).float() * l2.g\n    lin_grad(inp, l1, w1, b1)\n\n\nforward_and_backward(x_train, y_train)\n\n\n# Save for testing against later\ndef get_grad(x): return x.g.clone()\nchks = w1,w2,b1,b2,x_train\ngrads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))\n\nWe cheat a little bit and use PyTorch autograd to check our results.\n\ndef mkgrad(x): return x.clone().requires_grad_(True)\nptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))\n\n\ndef forward(inp, targ):\n    l1 = lin(inp, w12, b12)\n    l2 = relu(l1)\n    out = lin(l2, w22, b22)\n    return mse(out, targ)\n\n\nloss = forward(xt2, y_train)\nloss.backward()\n\n\nfor a,b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)"
  },
  {
    "objectID": "posts/course22p2/03_backprop.html#refactor-model",
    "href": "posts/course22p2/03_backprop.html#refactor-model",
    "title": "03 - The forward and backward passes",
    "section": "Refactor model",
    "text": "Refactor model\n\nLayers as classes\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g\n\n\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        #self.w.g = self.inp.t() @ self.out.g\n        #self.b.g = self.out.g.sum(0)\n\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.01)\ntest_close(ig, x_train.g, eps=0.01)\n\n\n\nModule.forward()\n\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n\n    def forward(self): raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n    def bwd(self): raise Exception('not implemented')\n\n\nclass Relu(Module):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n\n\nclass Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n    def forward(self, inp): return inp@self.w + self.b\n    def bwd(self, out, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse(Module):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.01)\ntest_close(ig, x_train.g, eps=0.01)\n\n\n\nAutograd\n\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Linear(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.w = torch.randn(n_in,n_out).requires_grad_()\n        self.b = torch.zeros(n_out).requires_grad_()\n    def forward(self, inp): return inp@self.w + self.b\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return F.mse_loss(x, targ[:,None])\n\n\nmodel = Model(m, nh, 1)\nloss = model(x_train, y_train)\nloss.backward()\n\n\nl0 = model.layers[0]\nl0.b.grad\n\ntensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html",
    "href": "posts/course22p2/02_meanshift.html",
    "title": "02 - Mean Shift",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page."
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html#create-data",
    "href": "posts/course22p2/02_meanshift.html#create-data",
    "title": "02 - Mean Shift",
    "section": "Create data",
    "text": "Create data\n\nn_clusters=6\nn_samples =250\n\nTo generate our data, we’re going to pick 6 random points, which we’ll call centroids, and for each point we’re going to generate 250 random points about it.\n\ncentroids = torch.rand(n_clusters, 2)*70-35\n\n\ncentroids\n\ntensor([[ 26.759,  29.050],\n        [ -8.200,  32.151],\n        [ -7.669,   7.063],\n        [-17.040,  20.555],\n        [ 30.854, -25.677],\n        [ 30.422,   6.551]])\n\n\n\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\n\n\n\n\n\n\nMore info for covariance matrix is on the lesson 9B.\n\n\n\n\n\n\n\ndef sample(m): return MultivariateNormal(m, torch.diag(tensor([5.,5.]))).sample((n_samples,))\n\n\nslices = [sample(c) for c in centroids]\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\nBelow we can see each centroid marked w/ X, and the coloring associated to each respective cluster.\n\ndef plot_data(centroids, data, n_samples, ax=None):\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)\n\n\nplot_data(centroids, data, n_samples)"
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html#mean-shift",
    "href": "posts/course22p2/02_meanshift.html#mean-shift",
    "title": "02 - Mean Shift",
    "section": "Mean shift",
    "text": "Mean shift\nMost people that have come across clustering algorithms have learnt about k-means. Mean shift clustering is a newer and less well-known approach, but it has some important advantages: * It doesn’t require selecting the number of clusters in advance, but instead just requires a bandwidth to be specified, which can be easily chosen automatically * It can handle clusters of any shape, whereas k-means (without using special extensions) requires that clusters be roughly ball shaped.\nThe algorithm is as follows: * For each data point x in the sample X, find the distance between that point x and every other point in X * Create weights for each point in X by using the Gaussian kernel of that point’s distance to x * This weighting approach penalizes points further away from x * The rate at which the weights fall to zero is determined by the bandwidth, which is the standard deviation of the Gaussian * Update x as the weighted average of all other points in X, weighted based on the previous step\nThis will iteratively push points that are close together even closer until they are next to each other.\n\nmidp = data.mean(0)\nmidp\n\ntensor([ 9.222, 11.604])\n\n\n\nplot_data([midp]*6, data, n_samples)\n\n\n\n\nSo here’s the definition of the gaussian kernel, which you may remember from high school… This person at the science march certainly remembered!\n\n\ndef gaussian(d, bw): return torch.exp(-0.5*((d/bw))**2) / (bw*math.sqrt(2*math.pi))\n\n\ndef plot_func(f):\n    x = torch.linspace(0,10,100)\n    plt.plot(x, f(x))\n\n\nplot_func(partial(gaussian, bw=2.5))\n\n\n\n\n:::{{ callout-note }} ## Partial functions are cool :::\n\npartial\n\nfunctools.partial\n\n\nIn our implementation, we choose the bandwidth to be 2.5.\nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n\ndef tri(d, i): return (-d+i).clamp_min(0)/i\n\n\nplot_func(partial(tri, i=8))\n\n\n\n\n\nX = data.clone()\nx = data[0]\n\n\nx\n\ntensor([26.204, 26.349])\n\n\n\nx.shape,X.shape,x[None].shape\n\n(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))\n\n\n\n(x[None]-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n(x-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n# rewrite using torch.einsum\ndist = ((x-X)**2).sum(1).sqrt()\ndist[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])\n\n\n\nweight = gaussian(dist, 2.5)\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([1500]), torch.Size([1500, 2]))\n\n\n\nweight[:,None].shape\n\ntorch.Size([1500, 1])\n\n\n\nweight[:,None]*X\n\ntensor([[    4.182,     4.205],\n        [    1.215,     1.429],\n        [    0.749,     0.706],\n        ...,\n        [    0.000,     0.000],\n        [    0.000,     0.000],\n        [    0.000,     0.000]])\n\n\n\ndef one_update(X):\n    for i, x in enumerate(X):\n        dist = torch.sqrt(((x-X)**2).sum(1))\n#         weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        X[i] = (weight[:,None]*X).sum(0)/weight.sum()\n\n\ndef meanshift(data):\n    X = data.clone()\n    for it in range(5): one_update(X)\n    return X\n\n\n%time X=meanshift(data)\n\nCPU times: user 453 ms, sys: 0 ns, total: 453 ms\nWall time: 452 ms\n\n\n\nplot_data(centroids+2, X, n_samples)"
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html#animation",
    "href": "posts/course22p2/02_meanshift.html#animation",
    "title": "02 - Mean Shift",
    "section": "Animation",
    "text": "Animation\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\ndef do_one(d):\n    if d: one_update(X)\n    ax.clear()\n    plot_data(centroids+2, X, n_samples, ax=ax)\n\n\n# create your own animation\nX = data.clone()\nfig,ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames=5, interval=500, repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html#gpu-batched-algorithm",
    "href": "posts/course22p2/02_meanshift.html#gpu-batched-algorithm",
    "title": "02 - Mean Shift",
    "section": "GPU batched algorithm",
    "text": "GPU batched algorithm\nTo truly accelerate the algorithm, we need to be performing updates on a batch of points per iteration, instead of just one as we were doing.\n\nbs=5\nX = data.clone()\nx = X[:bs]\nx.shape,X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a,b): return (((a[None]-b[:,None])**2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(X, x).shape\n\ntorch.Size([5, 1500])\n\n\n\nX[None,:].shape, x[:,None].shape, (X[None,:]-x[:,None]).shape\n\n(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))\n\n\n\nweight = gaussian(dist_b(X, x), 2)\nweight\n\ntensor([[    0.199,     0.030,     0.011,  ...,     0.000,     0.000,     0.000],\n        [    0.030,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.011,     0.009,     0.199,  ...,     0.000,     0.000,     0.000],\n        [    0.035,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.001,     0.004,     0.144,  ...,     0.000,     0.000,     0.000]])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([5, 1500]), torch.Size([1500, 2]))\n\n\n\nweight[...,None].shape, X[None].shape\n\n(torch.Size([5, 1500, 1]), torch.Size([1, 1500, 2]))\n\n\n\nnum = (weight[...,None]*X[None]).sum(1)\nnum.shape\n\ntorch.Size([5, 2])\n\n\n\nnum\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\ntorch.einsum('ij,jk->ik', weight, X)\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\nweight@X\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\ndiv = weight.sum(1, keepdim=True)\ndiv.shape\n\ntorch.Size([5, 1])\n\n\n\nnum/div\n\ntensor([[26.376, 27.692],\n        [26.101, 29.643],\n        [28.892, 28.990],\n        [26.071, 29.559],\n        [29.323, 29.685]])\n\n\n\ndef meanshift(data, bs=500):\n    n = len(data)\n    X = data.clone()\n    for it in range(5):\n        for i in range(0, n, bs):\n            s = slice(i, min(i+bs,n))\n            weight = gaussian(dist_b(X, X[s]), 2.5)\n#             weight = tri(dist_b(X, X[s]), 8)\n            div = weight.sum(1, keepdim=True)\n            X[s] = weight@X/div\n    return X\n\nAlthough each iteration still has to launch a new cuda kernel, there are now fewer iterations, and the acceleration from updating a batch of points more than makes up for it.\n\ndata = data.cuda()\n\n\nX = meanshift(data).cpu()\n\n\n%timeit -n 5 _=meanshift(data, 1250).cpu()\n\n2 ms ± 226 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nplot_data(centroids+2, X, n_samples)\n\n\n\n\nHomework: implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version.\nBonus: Implement it in APL too!\nSuper bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time.\nSuper super bonus: Publish a paper that describes it :D"
  },
  {
    "objectID": "posts/myfirstNN/2022-01-15-myFirstNN.html",
    "href": "posts/myfirstNN/2022-01-15-myFirstNN.html",
    "title": "Tiny bug world with neuro evolution (2018)",
    "section": "",
    "text": "This is from 2018, tutorials and trainings aside my first attempt for training a neural network. It does not have a traditional loss function or optimization method but it uses neuroevolution for training. I use p5.JS for the visualization and tensorflow.JS for the model.\nGithub Link is here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]