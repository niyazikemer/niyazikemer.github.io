[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Ai-Log",
    "section": "",
    "text": "07 - Convolutions\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n06 - Callbacks\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n05 - Datasets\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n04 - Minibatch Training\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n03 - The forward and backward passes\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n02 - Mean Shift\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\n01 - Matrix multiplication from foundations\n\n\n\n\n\n\n\nfastai\n\n\ncourse22p2\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNiyazi Kemer\n\n\n\n\n\n\n  \n\n\n\n\nTiny bug world with neuro evolution (2018)\n\n\n\n\n\n\n\nneuro evolution\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2022\n\n\nNiyazi Kemer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/course22p2/07_convolutions.html",
    "href": "posts/course22p2/07_convolutions.html",
    "title": "07 - Convolutions",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page."
  },
  {
    "objectID": "posts/course22p2/07_convolutions.html#creating-the-cnn",
    "href": "posts/course22p2/07_convolutions.html#creating-the-cnn",
    "title": "07 - Convolutions",
    "section": "Creating the CNN",
    "text": "Creating the CNN\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nbroken_cnn = nn.Sequential(\n    nn.Conv2d(1,30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30,10, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([16, 10, 28, 28])\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef conv(ni, nf, ks=3, stride=2, act=True):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n:::\nRefactoring parts of your neural networks like this makes it much less likely you’ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing.\n\nsimple_cnn = nn.Sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,16),           #2x2\n    conv(16,10, act=False), #1x1\n    nn.Flatten(),\n)\n\n\nsimple_cnn(xb).shape\n\ntorch.Size([16, 10])\n\n\n\nx_imgs = x_train.view(-1,1,28,28)\nxv_imgs = x_valid.view(-1,1,28,28)\ntrain_ds,valid_ds = Dataset(x_imgs, y_train),Dataset(xv_imgs, y_valid)\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef to_device(x, device=def_device):\n    if isinstance(x, torch.Tensor): return x.to(device)\n    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n    return type(x)(to_device(o, device) for o in x)\n\ndef collate_device(b): return to_device(default_collate(b))\n:::\n\nfrom torch import optim\n\nbs = 256\nlr = 0.4\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\nopt = optim.SGD(simple_cnn.parameters(), lr=lr)\n\n\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\n0 0.3630618950843811 0.8875999997138977\n1 0.16439641580581665 0.9496000003814697\n2 0.24622697901725768 0.9316000004768371\n3 0.25093305287361145 0.9335999998092651\n4 0.13128829071521758 0.9618000007629395\n\n\n\nopt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\n0 0.08451943595409393 0.9756999996185303\n1 0.08082638642787933 0.9777999995231629\n2 0.08050601842403411 0.9778999995231629\n3 0.08200360851287841 0.9773999995231628\n4 0.08405050563812255 0.9761999994277955\n\n\n\nUnderstanding Convolution Arithmetic\nIn an input of size 64x1x28x28 the axes are batch,channel,height,width. This is often represented as NCHW (where N refers to batch size). Tensorflow, on the other hand, uses NHWC axis order (aka “channels-last”). Channels-last is faster for many models, so recently it’s become more common to see this as an option in PyTorch too.\nWe have 1 input channel, 4 output channels, and a 3×3 kernel.\n\nsimple_cnn[0][0]\n\nConv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n\n\nconv1 = simple_cnn[0][0]\nconv1.weight.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\n\nconv1.bias.shape\n\ntorch.Size([4])\n\n\nThe receptive field is the area of an image that is involved in the calculation of a layer. conv-example.xlsx shows the calculation of two stride-2 convolutional layers using an MNIST digit. Here’s what we see if we click on one of the cells in the conv2 section, which shows the output of the second convolutional layer, and click trace precedents.\n\nThe blue highlighted cells are its precedents—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Click trace precedents again:\n\nIn this example, we have just two convolutional layers. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This is the receptive field\nThe deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer."
  },
  {
    "objectID": "posts/course22p2/07_convolutions.html#color-images",
    "href": "posts/course22p2/07_convolutions.html#color-images",
    "title": "07 - Convolutions",
    "section": "Color Images",
    "text": "Color Images\nA colour picture is a rank-3 tensor:\n\nfrom torchvision.io import read_image\n\n\nim = read_image('images/grizzly.jpg')\nim.shape\n\ntorch.Size([3, 1000, 846])\n\n\n\nshow_image(im.permute(1,2,0));\n\n\n\n\n\n_,axs = plt.subplots(1,3)\nfor bear,ax,color in zip(im,axs,('Reds','Greens','Blues')): show_image(255-bear, ax=ax, cmap=color)\n\n\n\n\n\nThese are then all added together, to produce a single number, for each grid location, for each output feature.\n\nWe have ch_out filters like this, so in the end, the result of our convolutional layer will be a batch of images with ch_out channels."
  },
  {
    "objectID": "posts/course22p2/07_convolutions.html#export--",
    "href": "posts/course22p2/07_convolutions.html#export--",
    "title": "07 - Convolutions",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html",
    "href": "posts/course22p2/04_minibatch_training.html",
    "title": "04 - Minibatch Training",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nEverything from scratch to pytorch\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#initial-setup",
    "href": "posts/course22p2/04_minibatch_training.html#initial-setup",
    "title": "04 - Minibatch Training",
    "section": "Initial setup",
    "text": "Initial setup\n\nData\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nmodel = Model(m, nh, 10)\npred = model(x_train)\npred.shape\n\ntorch.Size([50000, 10])\n\n\n\n\nCross entropy loss\nFirst, we will need to compute the softmax of our activations. This is defined by:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}\\]\nor more concisely:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}\\]\nIn practice, we will need the log of the softmax when we calculate the loss.\n\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\n\nlog_softmax(pred)\n\ntensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n        ...,\n        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)\n\n\nNote that the formula\n\\[\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)\\]\ngives a simplification when we compute the log softmax:\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\n\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )\\]\nwhere a is the maximum of the \\(x_{j}\\).\n\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\nThis way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us.\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\ntest_close(logsumexp(pred), pred.logsumexp(-1))\nsm_pred = log_softmax(pred)\nsm_pred\n\ntensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n        ...,\n        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)\n\n\nThe cross entropy loss for some target \\(x\\) and some prediction \\(p(x)\\) is given by:\n\\[ -\\sum x\\, \\log p(x) \\]\nBut since our \\(x\\)s are 1-hot encoded (actually, they’re just the integer indices), this can be rewritten as \\(-\\log(p_{i})\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\n\n\n\n\n\ncheck your model for this trick\n\n\n\nhttps://www.youtube.com/watch?v=vGdB4eI4KBs 1:38\n\n\n\ny_train[:3]\n\ntensor([5, 0, 4])\n\n\n\nsm_pred[0,5],sm_pred[1,0],sm_pred[2,4]\n\n(tensor(-2.20, grad_fn=<SelectBackward0>),\n tensor(-2.37, grad_fn=<SelectBackward0>),\n tensor(-2.36, grad_fn=<SelectBackward0>))\n\n\n\nsm_pred[[0,1,2], y_train[:3]]\n\ntensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)\n\n\n\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\n\nloss = nll(sm_pred, y_train)\nloss\n\ntensor(2.30, grad_fn=<NegBackward0>)\n\n\nThen use PyTorch’s implementation.\n\ntest_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)\n\nIn PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy.\n\ntest_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#basic-training-loop",
    "href": "posts/course22p2/04_minibatch_training.html#basic-training-loop",
    "title": "04 - Minibatch Training",
    "section": "Basic training loop",
    "text": "Basic training loop\nBasically the training loop repeats over the following steps: - get the output of the model on a batch of inputs - compare the output to the labels we have and compute a loss - calculate the gradients of the loss with respect to every parameter of the model - update said parameters with those gradients to make them a little bit better\n\nloss_func = F.cross_entropy\n\n\nbs=50                  # batch size\n\nxb = x_train[0:bs]     # a mini-batch from x\npreds = model(xb)      # predictions\npreds[0], preds.shape\n\n(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n torch.Size([50, 10]))\n\n\n\nyb = y_train[0:bs]\nyb\n\ntensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n        3, 9, 8, 5, 9, 3])\n\n\n\nloss_func(preds, yb)\n\ntensor(2.30, grad_fn=<NllLossBackward0>)\n\n\n\npreds.argmax(dim=1)\n\ntensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8,\n        3, 5, 9, 5, 9, 5])\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()\n:::\n\naccuracy(preds, yb)\n\ntensor(0.08)\n\n\n\nlr = 0.5   # learning rate\nepochs = 3 # how many epochs to train for\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')\n:::\n\nxb,yb = x_train[:bs],y_train[:bs]\npreds = model(xb)\nreport(loss_func(preds, yb), preds, yb)\n\n2.30, 0.08\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, 'weight'):\n                    l.weight -= l.weight.grad * lr\n                    l.bias   -= l.bias.grad   * lr\n                    l.weight.grad.zero_()\n                    l.bias  .grad.zero_()\n    report(loss, preds, yb)\n\n0.12, 0.98\n0.12, 0.94\n0.08, 0.96"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#using-parameters-and-optim",
    "href": "posts/course22p2/04_minibatch_training.html#using-parameters-and-optim",
    "title": "04 - Minibatch Training",
    "section": "Using parameters and optim",
    "text": "Using parameters and optim\n\nParameters\n\nm1 = nn.Module()\nm1.foo = nn.Linear(3,4)\nm1\n\nModule(\n  (foo): Linear(in_features=3, out_features=4, bias=True)\n)\n\n\n\nlist(m1.named_children())\n\n[('foo', Linear(in_features=3, out_features=4, bias=True))]\n\n\n\nm1.named_children()\n\n<generator object Module.named_children>\n\n\n\nlist(m1.parameters())\n\n[Parameter containing:\n tensor([[ 0.57,  0.43, -0.30],\n         [ 0.13, -0.32, -0.24],\n         [ 0.51,  0.04,  0.22],\n         [ 0.13, -0.17, -0.24]], requires_grad=True),\n Parameter containing:\n tensor([-0.01, -0.51, -0.39,  0.56], requires_grad=True)]\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.l1 = nn.Linear(n_in,nh)\n        self.l2 = nn.Linear(nh,n_out)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x): return self.l2(self.relu(self.l1(x)))\n\n\nmodel = MLP(m, nh, 10)\nmodel.l1\n\nLinear(in_features=784, out_features=50, bias=True)\n\n\n\nmodel\n\nMLP(\n  (l1): Linear(in_features=784, out_features=50, bias=True)\n  (l2): Linear(in_features=50, out_features=10, bias=True)\n  (relu): ReLU()\n)\n\n\n\nfor name,l in model.named_children(): print(f\"{name}: {l}\")\n\nl1: Linear(in_features=784, out_features=50, bias=True)\nl2: Linear(in_features=50, out_features=10, bias=True)\nrelu: ReLU()\n\n\n\nfor p in model.parameters(): print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n\ndef fit():\n    for epoch in range(epochs):\n        for i in range(0, n, bs):\n            s = slice(i, min(n,i+bs))\n            xb,yb = x_train[s],y_train[s]\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters(): p -= p.grad * lr\n                model.zero_grad()\n        report(loss, preds, yb)\n\n\nfit()\n\n0.19, 0.96\n0.11, 0.96\n0.04, 1.00\n\n\nBehind the scenes, PyTorch overrides the __setattr__ function in nn.Module so that the submodules you define are properly registered as parameters of the model.\n\nhow to create a module is below. kw = create module\n\nclass MyModule:\n    def __init__(self, n_in, nh, n_out):\n        self._modules = {}\n        self.l1 = nn.Linear(n_in,nh)\n        self.l2 = nn.Linear(nh,n_out)\n\n    def __setattr__(self,k,v):\n        if not k.startswith(\"_\"): self._modules[k] = v\n        super().__setattr__(k,v)\n\n    def __repr__(self): return f'{self._modules}'\n    \n    def parameters(self):\n        for l in self._modules.values(): yield from l.parameters()\n\n\nmdl = MyModule(m,nh,10)\nmdl\n\n{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}\n\n\n\nfor p in mdl.parameters(): print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n\n\nRegistering modules\n\nfrom functools import reduce\n\nWe can use the original layers approach, but we have to register the modules.\n\nlayers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]\n\n\n\n\n\n\n\nwhat is reduce\n\n\n\nreduce(lambda val,layer: layer(val), self.layers, x) is same as\nfor l in self.layers: x = l(x)\ncheck below\nclass SequentialModel(nn.Module):\n\n\n\nclass Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = layers\n        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n\n    def forward(self, x): return reduce(lambda val,layer: layer(val), self.layers, x)\n\n\nmodel = Model(layers)\nmodel\n\nModel(\n  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n  (layer_1): ReLU()\n  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nmodel(xb).shape\n\ntorch.Size([50, 10])\n\n\n\n\nnn.ModuleList\nnn.ModuleList does this for us.\n\nclass SequentialModel(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        \n    def forward(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nmodel = SequentialModel(layers)\nmodel\n\nSequentialModel(\n  (layers): ModuleList(\n    (0): Linear(in_features=784, out_features=50, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=50, out_features=10, bias=True)\n  )\n)\n\n\n\nfit()\n\n0.12, 0.96\n0.11, 0.96\n0.07, 0.98\n\n\n\n\nnn.Sequential\nnn.Sequential is a convenient class which does the same as the above:\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.16, 0.94\n0.13, 0.96\n0.08, 0.96\n\n\n(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))\n\n\n\nmodel\n\nSequential(\n  (0): Linear(in_features=784, out_features=50, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\n\noptim\n\nclass Optimizer():\n    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n\n    def step(self):\n        with torch.no_grad():\n            for p in self.params: p -= p.grad * self.lr\n\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nopt = Optimizer(model.parameters())\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.18, 0.94\n0.13, 0.96\n0.11, 0.94\n\n\nPyTorch already provides this exact functionality in optim.SGD (it also handles stuff like momentum, which we’ll look at later)\n\nfrom torch import optim\n\n\ndef get_model():\n    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n    return model, optim.SGD(model.parameters(), lr=lr)\n\n\nmodel,opt = get_model()\nloss_func(model(xb), yb)\n\ntensor(2.33, grad_fn=<NllLossBackward0>)\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.12, 0.98\n0.09, 0.98\n0.07, 0.98"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#dataset-and-dataloader",
    "href": "posts/course22p2/04_minibatch_training.html#dataset-and-dataloader",
    "title": "04 - Minibatch Training",
    "section": "Dataset and DataLoader",
    "text": "Dataset and DataLoader\n\nDataset\nIt’s clunky to iterate through minibatches of x and y values separately:\n    xb = x_train[s]\n    yb = y_train[s]\nInstead, let’s do these two steps together, by introducing a Dataset class:\n    xb,yb = train_ds[s]\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass Dataset():\n    def __init__(self, x, y): self.x,self.y = x,y\n    def __len__(self): return len(self.x)\n    def __getitem__(self, i): return self.x[i],self.y[i]\n:::\n\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\nassert len(train_ds)==len(x_train)\nassert len(valid_ds)==len(x_valid)\n\n\nxb,yb = train_ds[0:5]\nassert xb.shape==(5,28*28)\nassert yb.shape==(5,)\nxb,yb\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([5, 0, 4, 1, 9]))\n\n\n\nmodel,opt = get_model()\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        xb,yb = train_ds[i:min(n,i+bs)]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.17, 0.96\n0.11, 0.94\n0.09, 0.96\n\n\n\n\nDataLoader\nPreviously, our loop iterated over batches (xb, yb) like this:\nfor i in range(0, n, bs):\n    xb,yb = train_ds[i:min(n,i+bs)]\n    ...\nLet’s make our loop much cleaner, using a data loader:\nfor xb,yb in train_dl:\n    ...\n\nclass DataLoader():\n    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n    def __iter__(self):\n        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]\n\n\ntrain_dl = DataLoader(train_ds, bs)\nvalid_dl = DataLoader(valid_ds, bs)\n\n\nxb,yb = next(iter(valid_dl))\nxb.shape\n\ntorch.Size([50, 784])\n\n\n\nyb\n\ntensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n        8, 3, 7, 7, 8, 4])\n\n\n\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\nmodel,opt = get_model()\n\n\ndef fit():\n    for epoch in range(epochs):\n        for xb,yb in train_dl:\n            pred = model(xb)\n            loss = loss_func(pred, yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        report(loss, preds, yb)\n\n\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.11, 0.96\n0.09, 0.96\n0.06, 0.96\n\n\n(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))\n\n\n\n\nRandom sampling\nWe want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn’t be randomized.\n\nimport random\n\n\nclass Sampler():\n    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n    def __iter__(self):\n        res = list(range(self.n))\n        if self.shuffle: random.shuffle(res)\n        return iter(res)\n\n\nfrom itertools import islice\n\n\nss = Sampler(train_ds)\n\n\nit = iter(ss)\nfor o in range(5): print(next(it))\n\n0\n1\n2\n3\n4\n\n\n\nlist(islice(ss, 5))\n\n[0, 1, 2, 3, 4]\n\n\n\nss = Sampler(train_ds, shuffle=True)\nlist(islice(ss, 5))\n\n[11815, 32941, 21760, 21778, 35233]\n\n\n\nimport fastcore.all as fc\n\n\nclass BatchSampler():\n    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)\n\n\nbatchs = BatchSampler(ss, 4)\nlist(islice(batchs, 5))\n\n[[30214, 5339, 461, 9948],\n [8032, 20805, 16282, 13099],\n [26751, 2761, 552, 12897],\n [16714, 7294, 34658, 24330],\n [13836, 28629, 16552, 32028]]\n\n\n\ndef collate(b):\n    xs,ys = zip(*b)\n    return torch.stack(xs),torch.stack(ys)\n\n\nclass DataLoader():\n    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)\n\n\ntrain_samp = BatchSampler(Sampler(train_ds, shuffle=True ), bs)\nvalid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_samp)\nvalid_dl = DataLoader(valid_ds, batchs=valid_samp)\n\n\nxb,yb = next(iter(valid_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\nxb.shape,yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\nmodel,opt = get_model()\n\n\nfit()\n\n0.16, 0.08\n0.08, 0.04\n0.06, 0.08\n\n\n\n\nMultiprocessing DataLoader\n\nimport torch.multiprocessing as mp\nfrom fastcore.basics import store_attr\n\n\ntrain_ds[[3,6,8,1]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 1, 1, 0]))\n\n\n\ntrain_ds.__getitem__([3,6,8,1])\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 1, 1, 0]))\n\n\n\nfor o in map(train_ds.__getitem__, ([3,6],[8,1])): print(o)\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n\n\n\nclass DataLoader():\n    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n    def __iter__(self):\n        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\nit = iter(train_dl)\n\n\nxb,yb = next(it)\nxb.shape,yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\n\nPyTorch DataLoader\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n:::\n\ntrain_samp = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\nvalid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)\n\n\ntrain_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)\n\n\nmodel,opt = get_model()\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.10, 0.06\n0.10, 0.04\n0.27, 0.06\n\n\n(tensor(0.25, grad_fn=<NllLossBackward0>), tensor(0.94))\n\n\nPyTorch can auto-generate the BatchSampler for us:\n\ntrain_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)\n\nPyTorch can also generate the Sequential/RandomSamplers too:\n\ntrain_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\nvalid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)\n\n\nmodel,opt = get_model()\nfit()\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.21, 0.14\n0.15, 0.16\n0.05, 0.10\n\n\n(tensor(0.22, grad_fn=<NllLossBackward0>), tensor(0.96))\n\n\nOur dataset actually already knows how to sample a batch of indices all at once:\n\ntrain_ds[[4,6,7]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([9, 1, 3]))\n\n\n…that means that we can actually skip the batch_sampler and collate_fn entirely:\n\ntrain_dl = DataLoader(train_ds, sampler=train_samp)\nvalid_dl = DataLoader(valid_ds, sampler=valid_samp)\n\n\nxb,yb = next(iter(train_dl))\nxb.shape,yb.shape\n\n(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#validation",
    "href": "posts/course22p2/04_minibatch_training.html#validation",
    "title": "04 - Minibatch Training",
    "section": "Validation",
    "text": "Validation\nYou always should also have a validation set, in order to identify if you are overfitting.\nWe will calculate and print the validation loss at the end of each epoch.\n(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb,yb in train_dl:\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n        with torch.no_grad():\n            tot_loss,tot_acc,count = 0.,0.,0\n            for xb,yb in valid_dl:\n                pred = model(xb)\n                n = len(xb)\n                count += n\n                tot_loss += loss_func(pred,yb).item()*n\n                tot_acc  += accuracy (pred,yb).item()*n\n        print(epoch, tot_loss/count, tot_acc/count)\n    return tot_loss/count, tot_acc/count\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef get_dls(train_ds, valid_ds, bs, **kwargs):\n    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n:::\nNow, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:\n\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\nmodel,opt = get_model()\n\n\n%time loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)\n\n0 0.14236384611576797 0.958100004196167\n1 0.12564025789499284 0.9632000041007995\n2 0.1306914868950844 0.9645000052452087\n3 0.10988455526065082 0.9670000064373017\n4 0.11636362857650966 0.9678000068664551\nCPU times: user 10.5 s, sys: 16.3 s, total: 26.8 s\nWall time: 1.68 s"
  },
  {
    "objectID": "posts/course22p2/04_minibatch_training.html#export--",
    "href": "posts/course22p2/04_minibatch_training.html#export--",
    "title": "04 - Minibatch Training",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()\n\n:::{.callout-warning} # How to install the module MINIAI pip install -e '.[dev]' :::"
  },
  {
    "objectID": "posts/course22p2/06_foundations.html",
    "href": "posts/course22p2/06_foundations.html",
    "title": "06 - Callbacks",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nCallbacks and dunder methods"
  },
  {
    "objectID": "posts/course22p2/06_foundations.html#callbacks",
    "href": "posts/course22p2/06_foundations.html#callbacks",
    "title": "06 - Callbacks",
    "section": "Callbacks",
    "text": "Callbacks\n\nCallbacks as GUI events\n\nimport ipywidgets as widgets\n\nFrom the ipywidget docs:\n\nthe button widget is used to handle mouse clicks. The on_click method of the Button can be used to register function to be called when the button is clicked\n\n\nw = widgets.Button(description='Click me')\n\n\nw\n\n\n\n\n\ndef f(o): print('hi')\n\n\nw.on_click(f)\n\nNB: When callbacks are used in this way they are often called “events”.\n\n\nCreating your own callback\n\nfrom time import sleep\n\n\ndef slow_calculation():\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1)\n    return res\n\n\nslow_calculation()\n\n30\n\n\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1)\n        if cb: cb(i)\n    return res\n\n\ndef show_progress(epoch): print(f\"Awesome! We've finished epoch {epoch}!\")\n\n\nslow_calculation(show_progress)\n\nAwesome! We've finished epoch 0!\nAwesome! We've finished epoch 1!\nAwesome! We've finished epoch 2!\nAwesome! We've finished epoch 3!\nAwesome! We've finished epoch 4!\n\n\n30\n\n\n\n\nLambdas and partials\n\nslow_calculation(lambda o: print(f\"Awesome! We've finished epoch {o}!\"))\n\nAwesome! We've finished epoch 0!\nAwesome! We've finished epoch 1!\nAwesome! We've finished epoch 2!\nAwesome! We've finished epoch 3!\nAwesome! We've finished epoch 4!\n\n\n30\n\n\n\ndef show_progress(exclamation, epoch): print(f\"{exclamation}! We've finished epoch {epoch}!\")\n\n\nslow_calculation(lambda o: show_progress(\"OK I guess\", o))\n\nOK I guess! We've finished epoch 0!\nOK I guess! We've finished epoch 1!\nOK I guess! We've finished epoch 2!\nOK I guess! We've finished epoch 3!\nOK I guess! We've finished epoch 4!\n\n\n30\n\n\n\ndef make_show_progress(exclamation):\n    def _inner(epoch): print(f\"{exclamation}! We've finished epoch {epoch}!\")\n    return _inner\n\n\nslow_calculation(make_show_progress(\"Nice!\"))\n\nNice!! We've finished epoch 0!\nNice!! We've finished epoch 1!\nNice!! We've finished epoch 2!\nNice!! We've finished epoch 3!\nNice!! We've finished epoch 4!\n\n\n30\n\n\n\nfrom functools import partial\n\n\nslow_calculation(partial(show_progress, \"OK I guess\"))\n\nOK I guess! We've finished epoch 0!\nOK I guess! We've finished epoch 1!\nOK I guess! We've finished epoch 2!\nOK I guess! We've finished epoch 3!\nOK I guess! We've finished epoch 4!\n\n\n30\n\n\n\nf2 = partial(show_progress, \"OK I guess\")\n\n\n\nCallbacks as callable classes\n\nclass ProgressShowingCallback():\n    def __init__(self, exclamation=\"Awesome\"): self.exclamation = exclamation\n    def __call__(self, epoch): print(f\"{self.exclamation}! We've finished epoch {epoch}!\")\n\n\ncb = ProgressShowingCallback(\"Just super\")\n\n\nslow_calculation(cb)\n\nJust super! We've finished epoch 0!\nJust super! We've finished epoch 1!\nJust super! We've finished epoch 2!\nJust super! We've finished epoch 3!\nJust super! We've finished epoch 4!\n\n\n30\n\n\n\n\nMultiple callback funcs; *args and **kwargs\n\ndef f(*a, **b): print(f\"args: {a}; kwargs: {b}\")\n\n\nf(3, 'a', thing1=\"hello\")\n\nargs: (3, 'a'); kwargs: {'thing1': 'hello'}\n\n\n\ndef g(a,b,c=0): print(a,b,c)\n\n\nargs = [1,2]\nkwargs = {'c':3}\ng(*args, **kwargs)\n\n1 2 3\n\n\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb: cb.before_calc(i)\n        res += i*i\n        sleep(1)\n        if cb: cb.after_calc(i, val=res)\n    return res\n\n\nclass PrintStepCallback():\n    def before_calc(self, *args, **kwargs): print(f\"About to start\")\n    def after_calc (self, *args, **kwargs): print(f\"Done step\")\n\n\nslow_calculation(PrintStepCallback())\n\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\n\n\n30\n\n\n\nclass PrintStatusCallback():\n    def __init__(self): pass\n    def before_calc(self, epoch, **kwargs): print(f\"About to start: {epoch}\")\n    def after_calc (self, epoch, val, **kwargs): print(f\"After {epoch}: {val}\")\n\n\nslow_calculation(PrintStatusCallback())\n\nAbout to start: 0\nAfter 0: 0\nAbout to start: 1\nAfter 1: 1\nAbout to start: 2\nAfter 2: 5\nAbout to start: 3\nAfter 3: 14\nAbout to start: 4\nAfter 4: 30\n\n\n30\n\n\n\n\nModifying behavior\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb and hasattr(cb,'before_calc'): cb.before_calc(i)\n        res += i*i\n        sleep(1)\n        if cb and hasattr(cb,'after_calc'):\n            if cb.after_calc(i, res):\n                print(\"stopping early\")\n                break\n    return res\n\n\nclass PrintAfterCallback():\n    def after_calc (self, epoch, val):\n        print(f\"After {epoch}: {val}\")\n        if val>10: return True\n\n\nslow_calculation(PrintAfterCallback())\n\nAfter 0: 0\nAfter 1: 1\nAfter 2: 5\nAfter 3: 14\nstopping early\n\n\n14\n\n\n\nclass SlowCalculator():\n    def __init__(self, cb=None): self.cb,self.res = cb,0\n    \n    def callback(self, cb_name, *args):\n        if not self.cb: return\n        cb = getattr(self.cb,cb_name, None)\n        if cb: return cb(self, *args)\n\n    def calc(self):\n        for i in range(5):\n            self.callback('before_calc', i)\n            self.res += i*i\n            sleep(1)\n            if self.callback('after_calc', i):\n                print(\"stopping early\")\n                break\n\n\nclass ModifyingCallback():\n    def after_calc (self, calc, epoch):\n        print(f\"After {epoch}: {calc.res}\")\n        if calc.res>10: return True\n        if calc.res<3: calc.res = calc.res*2\n\n\ncalculator = SlowCalculator(ModifyingCallback())\n\n\ncalculator.calc()\ncalculator.res\n\nAfter 0: 0\nAfter 1: 1\nAfter 2: 6\nAfter 3: 15\nstopping early\n\n\n15"
  },
  {
    "objectID": "posts/course22p2/06_foundations.html#dunder__-thingies",
    "href": "posts/course22p2/06_foundations.html#dunder__-thingies",
    "title": "06 - Callbacks",
    "section": "__dunder__ thingies",
    "text": "__dunder__ thingies\nAnything that looks like __this__ is, in some way, special. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call __init__. These are defined as part of the python data model.\nFor instance, if python sees +, then it will call the special method __add__. If you try to display an object in Jupyter (or lots of other places in Python) it will call __repr__.\n\nclass SloppyAdder():\n    def __init__(self,o): self.o=o\n    def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01)\n    def __repr__(self): return str(self.o)\n\n\na = SloppyAdder(1)\nb = SloppyAdder(2)\na+b\n\n3.01\n\n\nSpecial methods you should probably know about (see data model link above) are:\n\n__getitem__\n__getattr__\n__setattr__\n__del__\n__init__\n__new__\n__enter__\n__exit__\n__len__\n__repr__\n__str__\n\n\n__getattr__ and getattr\n\nclass A: a,b=1,2\n\n\na = A()\n\n\na.b\n\n2\n\n\n\ngetattr(a, 'b')\n\n2\n\n\n\ngetattr(a, 'b' if random.random()>0.5 else 'a')\n\n2\n\n\n\nclass B:\n    a,b=1,2\n    def __getattr__(self, k):\n        if k[0]=='_': raise AttributeError(k)\n        return f'Hello from {k}'\n\n\nb = B()\n\n\nb.a\n\n1\n\n\n\nb.foo\n\n'Hello from foo'"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html",
    "href": "posts/course22p2/05_datasets.html",
    "title": "05 - Datasets",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page.\nDatasets, Dataloaders\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#hugging-face-datasets",
    "href": "posts/course22p2/05_datasets.html#hugging-face-datasets",
    "title": "05 - Datasets",
    "section": "Hugging Face Datasets",
    "text": "Hugging Face Datasets\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nprint(ds_builder.info.description)\n\n\nds_builder.info.features\n\n\nds_builder.info.splits\n\n\ndsd = load_dataset(name)\ndsd\n\n\ntrain,test = dsd['train'],dsd['test']\ntrain[0]\n\n\nx,y = ds_builder.info.features\n\n\nx,y\n\n\nx,y = 'image','label'\nimg = train[0][x]\nimg\n\n\nxb = train[:5][x]\nyb = train[:5][y]\nyb\n\n\nfeaty = train.features[y]\nfeaty\n\n\nfeaty.int2str(yb)\n\n\ntrain['label'][:5]\n\n\ndef collate_fn(b):\n    return {x:torch.stack([TF.to_tensor(o[x]) for o in b]),\n            y:tensor([o[y] for o in b])}\n\n\ndl = DataLoader(train, collate_fn=collate_fn, batch_size=16)\nb = next(iter(dl))\nb[x].shape,b[y]\n\n\ndef transforms(b):\n    b[x] = [TF.to_tensor(o) for o in b[x]]\n    return b\n\n\ntds = train.with_transform(transforms)\ndl = DataLoader(tds, batch_size=16)\nb = next(iter(dl))\nb[x].shape,b[y]\n\n\ndef _transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef inplace(f):\n    def _f(b):\n        f(b)\n        return b\n    return _f\n:::\n\ntransformi = inplace(_transformi)\n\n\nr = train.with_transform(transformi)[0]\nr[x].shape,r[y]\n\n\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n\ntdsf = train.with_transform(transformi)\nr = tdsf[0]\nr[x].shape,r[y]\n\n\nd = dict(a=1,b=2,c=3)\nig = itemgetter('a','c')\nig(d)\n\n\nclass D:\n    def __getitem__(self, k): return 1 if k=='a' else 2 if k=='b' else 3\n\n\nd = D()\nig(d)\n\n\nlist(tdsf.features)\n\n\nbatch = dict(a=[1],b=[2]), dict(a=[3],b=[4])\ndefault_collate(batch)"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#miniai",
    "href": "posts/course22p2/05_datasets.html#miniai",
    "title": "05 - Datasets",
    "section": "miniai",
    "text": "miniai\naround 1:17 there is an explanation of miniai and its installation"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#plotting-images",
    "href": "posts/course22p2/05_datasets.html#plotting-images",
    "title": "05 - Datasets",
    "section": "Plotting images",
    "text": "Plotting images\n\nb = next(iter(dl))\nxb = b['image']\nimg = xb[0]\nplt.imshow(img[0]);"
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#kwargs",
    "href": "posts/course22p2/05_datasets.html#kwargs",
    "title": "05 - Datasets",
    "section": "**kwargs",
    "text": "**kwargs\n@fc.delegates makes imshow kwargs visible. Great."
  },
  {
    "objectID": "posts/course22p2/05_datasets.html#export--",
    "href": "posts/course22p2/05_datasets.html#export--",
    "title": "05 - Datasets",
    "section": "Export -",
    "text": "Export -\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html",
    "href": "posts/course22p2/01_matmul.html",
    "title": "01 - Matrix multiplication from foundations",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page."
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#matrix-multiplication-from-foundations",
    "href": "posts/course22p2/01_matmul.html#matrix-multiplication-from-foundations",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Matrix multiplication from foundations",
    "text": "Matrix multiplication from foundations\nThe foundations we’ll assume throughout this course are:\n\nPython\nmatplotlib\nThe Python standard library\nJupyter notebooks and nbdev\n\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#get-data",
    "href": "posts/course22p2/01_matmul.html#get-data",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-rw-r-- 1 niyazi niyazi 17051982 Şub  5 11:17 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\n\n\n\n\n\nChoosing colormaps in Matplotlib\n\n\n\nhttps://matplotlib.org/stable/tutorials/colors/colormaps.html\n\n\n\n# this cahanges the colormap\nmpl.rcParams['image.cmap'] = 'magma'\n# mpl.pyplot == plt.imshow\nmpl.pyplot.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice>\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\n\n\n\n\n\nIter-callable usage\n\n\n\nif a callable passed to a iter(lambda) then it runs again and again until it thit the sentinel that is [] at this time\n\n\n\niter(lambda: list(islice(it, 28)), [])\n\n<callable_iterator>\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#matrix-and-tensor",
    "href": "posts/course22p2/01_matmul.html#matrix-and-tensor",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\n\nimgs[0,20,15]\n\ntensor(0.99)\n\n\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#random-numbers",
    "href": "posts/course22p2/01_matmul.html#random-numbers",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Random numbers",
    "text": "Random numbers\nQuantum random numbers from The Australian National University https://qrng.anu.edu.au/\nCloudFlare random number generator. https://blog.cloudflare.com/randomness-101-lavarand-in-production/\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.93])\nIn child: tensor([0.93])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n1.55 ms ± 8.41 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n%timeit -n 10 torch.randn(784,10)\n\n26.9 µs ± 13.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#matrix-multiplication",
    "href": "posts/course22p2/01_matmul.html#matrix-multiplication",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\n\n\n\n\n\n\nFor a better visibility change the linewidth from 80 to 140\n\n\n\n\n\n\n\ntorch.set_printoptions(precision=2, linewidth=80, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,\n          -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,\n          -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,\n          13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,\n           3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,\n           5.28]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\n\n\n\n\n\nThe same content with 140 characters\n\n\n\n\n\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\n\n%time _=matmul(m1, m2)\n\nCPU times: user 227 ms, sys: 0 ns, total: 227 ms\nWall time: 227 ms\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#numba",
    "href": "posts/course22p2/01_matmul.html#numba",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Numba",
    "text": "Numba\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 47.3 ms, sys: 3.69 ms, total: 50.9 ms\nWall time: 50.8 ms\n\n\n20.0\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 16 µs, sys: 0 ns, total: 16 µs\nWall time: 18.1 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n\n%timeit -n 50 matmul(m1a,m2a)\n\n148 µs ± 14.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#elementwise-ops",
    "href": "posts/course22p2/01_matmul.html#elementwise-ops",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nnk allways right to left no operator precedence\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\n\na + b\n\ntensor([12., 14.,  3.])\n\n\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nFrobenius norm:\n\\[\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}\\]\nHint: you don’t normally need to write equations in LaTeX yourself, instead, you can click ‘edit’ in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click “Download: Other formats” in the top right, then “Download source”; rename the downloaded file to end in .tgz if it doesn’t already, and you should find the source there, including the equations to copy and paste. This is the source LaTeX that I pasted to render the equation above:\n$$\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}$$\n\nsf = (m*m).sum()\nsf\n\ntensor(285.)\n\n\n\nsf.sqrt()\n\ntensor(16.88)\n\n\n\nm[2,:],m[:,2]\n\n(tensor([7., 8., 9.]), tensor([3., 6., 9.]))\n\n\n\nm[2]\n\ntensor([7., 8., 9.])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n279 µs ± 2.53 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = torch.dot(a[i,:], b[:,j])\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n236 µs ± 3.12 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#broadcasting",
    "href": "posts/course22p2/01_matmul.html#broadcasting",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Broadcasting",
    "text": "Broadcasting\nThe term broadcasting describes how arrays with different shapes are treated during arithmetic operations.\nFrom the Numpy Documentation:\nThe term broadcasting describes how numpy treats arrays with \ndifferent shapes during arithmetic operations. Subject to certain \nconstraints, the smaller array is “broadcast” across the larger \narray so that they have compatible shapes. Broadcasting provides a \nmeans of vectorizing array operations so that looping occurs in C\ninstead of Python. It does this without making needless copies of \ndata and usually leads to efficient algorithm implementations.\nIn addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\nThis section was adapted from Chapter 4 of the fast.ai Computational Linear Algebra course.\n\nBroadcasting with a scalar\n\na\n\ntensor([10.,  6., -4.])\n\n\n\na > 0\n\ntensor([ True,  True, False])\n\n\nHow are we able to do a > 0? 0 is being broadcast to have the same dimensions as a.\nFor instance you can normalize our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar), using broadcasting.\nOther examples of broadcasting with a scalar:\n\na + 1\n\ntensor([11.,  7., -3.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\n2*m\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\n\nBroadcasting a vector to a matrix\nAlthough broadcasting a scalar is an idea that dates back to APL, the more powerful idea of broadcasting across higher rank tensors comes from a little known language called Yorick.\nWe can also broadcast a vector to a matrix:\n\nc = tensor([10.,20,30]); c\n\ntensor([10., 20., 30.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nm.shape,c.shape\n\n(torch.Size([3, 3]), torch.Size([3]))\n\n\n\nm + c\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nc + m\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nt = c.expand_as(m)\n\n\nt\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\n\nm + t\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\nWe don’t really copy the rows, but it looks as if we did. In fact, the rows are given a stride of 0.\n\nt.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n\n\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\nYou can index with the special value [None] or use unsqueeze() to convert a 1-dimensional array into a 2-dimensional array (although one of those dimensions has value 1).\n\nc.shape\n\ntorch.Size([3])\n\n\n\nc.unsqueeze(0), c[None, :]\n\n(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]))\n\n\n\nc.shape, c.unsqueeze(0).shape\n\n(torch.Size([3]), torch.Size([1, 3]))\n\n\n\nc.unsqueeze(1), c[:, None]\n\n(tensor([[10.],\n         [20.],\n         [30.]]),\n tensor([[10.],\n         [20.],\n         [30.]]))\n\n\n\nc.shape, c.unsqueeze(1).shape\n\n(torch.Size([3]), torch.Size([3, 1]))\n\n\nYou can always skip trailling ‘:’s. And’…’ means ‘all preceding dimensions’\n\nc[None].shape,c[...,None].shape\n\n(torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\nc[:,None].expand_as(m)\n\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n\n\n\nm + c[:,None]\n\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n\n\n\nm + c[None,:]\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\n\nBroadcasting Rules\n\nc[None,:]\n\ntensor([[10., 20., 30.]])\n\n\n\nc[None,:].shape\n\ntorch.Size([1, 3])\n\n\n\nc[:,None]\n\ntensor([[10.],\n        [20.],\n        [30.]])\n\n\n\nc[:,None].shape\n\ntorch.Size([3, 1])\n\n\n\nc[None,:] * c[:,None]\n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\n\nc[None] > c[:,None]\n\ntensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])\n\n\n\nm*m\n\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n\n\nWhen operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when\n\nthey are equal, or\none of them is 1, in which case that dimension is broadcasted to make it the same size\n\nArrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\nImage  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\nThe numpy documentation includes several examples of what dimensions can and can not be broadcast together."
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#matmul-with-broadcasting",
    "href": "posts/course22p2/01_matmul.html#matmul-with-broadcasting",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\ndigit is the first digit in the dataset\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\nm2.shape\n\ntorch.Size([784, 10])\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n70.1 µs ± 1.97 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nOur time has gone from ~500ms to <0.1ms, an over 5000x improvement! We can run on the whole dataset now.\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 6.59 s, sys: 200 ms, total: 6.79 s\nWall time: 663 ms"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#einstein-summation",
    "href": "posts/course22p2/01_matmul.html#einstein-summation",
    "title": "01 - Matrix multiplication from foundations",
    "section": "Einstein summation",
    "text": "Einstein summation\nEinstein summation (einsum) is a compact representation for combining products and sums in a general way. The key rules are:\n\nRepeating letters between input arrays means that values along those axes will be multiplied together.\nOmitting a letter from the output means that values along that axis will be summed.\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\n# c[i,j] += a[i,k] * b[k,j]\n# c[i,j] = (a[i,:] * b[:,j]).sum()\nmr = torch.einsum('ik,kj->ikj', m1, m2)\nmr.shape\n\ntorch.Size([5, 784, 10])\n\n\n\nmr.sum(1)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ntorch.einsum('ik,kj->ij', m1, m2)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n\n\ntest_close(tr, matmul(x_train, weights), eps=1e-3)\n\n\n%timeit -n 5 _=matmul(x_train, weights)\n\n15.1 ms ± 176 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#pytorch-op",
    "href": "posts/course22p2/01_matmul.html#pytorch-op",
    "title": "01 - Matrix multiplication from foundations",
    "section": "pytorch op",
    "text": "pytorch op\nWe can use pytorch’s function or operator directly for matrix multiplication.\n\ntest_close(tr, x_train@weights, eps=1e-3)\n\n\n%timeit -n 5 _=torch.matmul(x_train, weights)\n\n15.2 ms ± 96.2 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "posts/course22p2/01_matmul.html#cuda",
    "href": "posts/course22p2/01_matmul.html#cuda",
    "title": "01 - Matrix multiplication from foundations",
    "section": "CUDA",
    "text": "CUDA\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\nres = torch.zeros(ar, bc)\nmatmul((0,0), m1, m2, res)\nres\n\ntensor([[-10.94,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00]])\n\n\n\ndef launch_kernel(kernel, grid_x, grid_y, *args, **kwargs):\n    for i in range(grid_x):\n        for j in range(grid_y): kernel((i,j), *args, **kwargs)\n\n\nres = torch.zeros(ar, bc)\nlaunch_kernel(matmul, ar, bc, m1, m2, res)\nres\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nfrom numba import cuda\n\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\n@cuda.jit\ndef matmul(a,b,c):\n    i, j = cuda.grid(2)\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\nr = np.zeros(tr.shape)\nm1g,m2g,rg = map(cuda.to_device, (x_train,weights,r))\n\n\nr.shape\n\n(50000, 10)\n\n\n\nTPB = 16\nrr,rc = r.shape\nblockspergrid = (math.ceil(rr / TPB), math.ceil(rc / TPB))\nblockspergrid\n\n(3125, 1)\n\n\n\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\ntest_close(tr, r, eps=1e-3)\n\n\n%%timeit -n 10\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\n\n3.61 ms ± 708 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nm1c,m2c = x_train.cuda(),weights.cuda()\n\n\nr=(m1c@m2c).cpu()\n\n\n%timeit -n 10 r=(m1c@m2c).cpu()\n\n458 µs ± 93.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nOur broadcasting version was >500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!"
  },
  {
    "objectID": "posts/course22p2/03_backprop.html",
    "href": "posts/course22p2/03_backprop.html",
    "title": "03 - The forward and backward passes",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page."
  },
  {
    "objectID": "posts/course22p2/03_backprop.html#the-forward-and-backward-passes",
    "href": "posts/course22p2/03_backprop.html#the-forward-and-backward-passes",
    "title": "03 - The forward and backward passes",
    "section": "The forward and backward passes",
    "text": "The forward and backward passes\n\n\n\ncalculus.png\n\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nfrom pathlib import Path\nfrom torch import tensor\nfrom fastcore.test import test_close\ntorch.manual_seed(42)\n\nmpl.rcParams['image.cmap'] = 'gray'\ntorch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\nnp.set_printoptions(precision=2, linewidth=125)\n\npath_data = Path('data')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
  },
  {
    "objectID": "posts/course22p2/03_backprop.html#foundations-version",
    "href": "posts/course22p2/03_backprop.html#foundations-version",
    "title": "03 - The forward and backward passes",
    "section": "Foundations version",
    "text": "Foundations version\n\nBasic architecture\n\nn,m = x_train.shape\nc = y_train.max()+1\nn,m,c\n\n(50000, 784, tensor(10))\n\n\n\n# num hidden\nnh = 50\n\n\nw1 = torch.randn(m,nh)\nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)\nb2 = torch.zeros(1)\n\n\ndef lin(x, w, b): return x@w + b\n\n\nt = lin(x_valid, w1, b1)\nt.shape\n\ntorch.Size([10000, 50])\n\n\n\ndef relu(x): return x.clamp_min(0.)\n\n\nt = relu(t)\nt\n\ntensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n        ...,\n        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])\n\n\n\ndef model(xb):\n    l1 = lin(xb, w1, b1)\n    l2 = relu(l1)\n    return lin(l2, w2, b2)\n\n\nres = model(x_valid)\nres.shape\n\ntorch.Size([10000, 1])\n\n\n\n\nLoss function: MSE\n(Of course, mse is not a suitable loss function for multi-class classification; we’ll use a better loss function soon. We’ll use mse for now to keep things simple.)\n\nres.shape,y_valid.shape\n\n(torch.Size([10000, 1]), torch.Size([10000]))\n\n\n\n(res-y_valid).shape\n\ntorch.Size([10000, 10000])\n\n\nWe need to get rid of that trailing (,1), in order to use mse.\n\nres[:,0].shape\n\ntorch.Size([10000])\n\n\n\nres.squeeze().shape\n\ntorch.Size([10000])\n\n\n\n(res[:,0]-y_valid).shape\n\ntorch.Size([10000])\n\n\n\ny_train,y_valid = y_train.float(),y_valid.float()\n\npreds = model(x_train)\npreds.shape\n\ntorch.Size([50000, 1])\n\n\n\ndef mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n\n\nmse(preds, y_train)\n\ntensor(4308.76)\n\n\n\n\nGradients and backward pass\n\nfrom sympy import symbols,diff\nx,y = symbols('x y')\ndiff(x**2, x)\n\n\\(\\displaystyle 2 x\\)\n\n\n\ndiff(3*x**2+9, x)\n\n\\(\\displaystyle 6 x\\)\n\n\n\n\n\n\n\n\nSteps of the back-propagation\n\n\n\n\n\n\n\n\n\nchain.png\n\n\n\n\n\n\n\n\nFurther reading\n\n\n\n\ncheck here too for a more detailed explanation of backpropagation. Same thing but includes extra-steps. https://course19.fast.ai/videos/?lesson=8m around 1:53 code here too https://github.com/fastai/course-v3/blob/master/nbs/dl2/02_fully_connected.ipynb\n\nGradient of the relu.\nmse grad( gradient of the loss)\ngradient of the linear layer.\n\n\nThe Intuitive Notion of the Chain Rule\n\nhttps://webspace.ship.edu/msrenault/geogebracalculus/derivative_intuitive_chain_rule.html\n\nThe Matrix Calculus You Need For Deep Learning\n\nhttps://explained.ai/matrix-calculus/\n\n\n\n\n\n# keywords python debugger debug\n\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    #import pdb; pdb.set_trace()\n    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n    b.g = out.g.sum(0)\n\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = lin(inp, w1, b1)\n    l2 = relu(l1)\n    out = lin(l2, w2, b2)\n    diff = out[:,0]-targ\n    loss = diff.pow(2).mean()\n    \n    # backward pass:\n    out.g = 2.*diff[:,None] / inp.shape[0]\n    lin_grad(l2, out, w2, b2)\n    l1.g = (l1>0).float() * l2.g\n    lin_grad(inp, l1, w1, b1)\n\n\nforward_and_backward(x_train, y_train)\n\n\n# Save for testing against later\ndef get_grad(x): return x.g.clone()\nchks = w1,w2,b1,b2,x_train\ngrads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))\n\nWe cheat a little bit and use PyTorch autograd to check our results.\n\ndef mkgrad(x): return x.clone().requires_grad_(True)\nptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))\n\n\ndef forward(inp, targ):\n    l1 = lin(inp, w12, b12)\n    l2 = relu(l1)\n    out = lin(l2, w22, b22)\n    return mse(out, targ)\n\n\nloss = forward(xt2, y_train)\nloss.backward()\n\n\nfor a,b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)"
  },
  {
    "objectID": "posts/course22p2/03_backprop.html#refactor-model",
    "href": "posts/course22p2/03_backprop.html#refactor-model",
    "title": "03 - The forward and backward passes",
    "section": "Refactor model",
    "text": "Refactor model\n\nLayers as classes\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g\n\n\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        #self.w.g = self.inp.t() @ self.out.g\n        #self.b.g = self.out.g.sum(0)\n\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.01)\ntest_close(ig, x_train.g, eps=0.01)\n\n\n\nModule.forward()\n\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n\n    def forward(self): raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n    def bwd(self): raise Exception('not implemented')\n\n\nclass Relu(Module):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n\n\nclass Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n    def forward(self, inp): return inp@self.w + self.b\n    def bwd(self, out, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse(Module):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.01)\ntest_close(ig, x_train.g, eps=0.01)\n\n\n\nAutograd\n\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Linear(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.w = torch.randn(n_in,n_out).requires_grad_()\n        self.b = torch.zeros(n_out).requires_grad_()\n    def forward(self, inp): return inp@self.w + self.b\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return F.mse_loss(x, targ[:,None])\n\n\nmodel = Model(m, nh, 1)\nloss = model(x_train, y_train)\nloss.backward()\n\n\nl0 = model.layers[0]\nl0.b.grad\n\ntensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html",
    "href": "posts/course22p2/02_meanshift.html",
    "title": "02 - Mean Shift",
    "section": "",
    "text": "This is not my content it’s a part of Fastai’s From Deep Learning Foundations to Stable Diffusion course. I add some notes for me to understand better thats all. For the source check Fastai course page."
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html#create-data",
    "href": "posts/course22p2/02_meanshift.html#create-data",
    "title": "02 - Mean Shift",
    "section": "Create data",
    "text": "Create data\n\nn_clusters=6\nn_samples =250\n\nTo generate our data, we’re going to pick 6 random points, which we’ll call centroids, and for each point we’re going to generate 250 random points about it.\n\ncentroids = torch.rand(n_clusters, 2)*70-35\n\n\ncentroids\n\ntensor([[ 26.759,  29.050],\n        [ -8.200,  32.151],\n        [ -7.669,   7.063],\n        [-17.040,  20.555],\n        [ 30.854, -25.677],\n        [ 30.422,   6.551]])\n\n\n\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\n\n\n\n\n\n\nMore info for covariance matrix is on the lesson 9B.\n\n\n\n\n\n\n\ndef sample(m): return MultivariateNormal(m, torch.diag(tensor([5.,5.]))).sample((n_samples,))\n\n\nslices = [sample(c) for c in centroids]\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\nBelow we can see each centroid marked w/ X, and the coloring associated to each respective cluster.\n\ndef plot_data(centroids, data, n_samples, ax=None):\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)\n\n\nplot_data(centroids, data, n_samples)"
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html#mean-shift",
    "href": "posts/course22p2/02_meanshift.html#mean-shift",
    "title": "02 - Mean Shift",
    "section": "Mean shift",
    "text": "Mean shift\nMost people that have come across clustering algorithms have learnt about k-means. Mean shift clustering is a newer and less well-known approach, but it has some important advantages: * It doesn’t require selecting the number of clusters in advance, but instead just requires a bandwidth to be specified, which can be easily chosen automatically * It can handle clusters of any shape, whereas k-means (without using special extensions) requires that clusters be roughly ball shaped.\nThe algorithm is as follows: * For each data point x in the sample X, find the distance between that point x and every other point in X * Create weights for each point in X by using the Gaussian kernel of that point’s distance to x * This weighting approach penalizes points further away from x * The rate at which the weights fall to zero is determined by the bandwidth, which is the standard deviation of the Gaussian * Update x as the weighted average of all other points in X, weighted based on the previous step\nThis will iteratively push points that are close together even closer until they are next to each other.\n\nmidp = data.mean(0)\nmidp\n\ntensor([ 9.222, 11.604])\n\n\n\nplot_data([midp]*6, data, n_samples)\n\n\n\n\nSo here’s the definition of the gaussian kernel, which you may remember from high school… This person at the science march certainly remembered!\n\n\ndef gaussian(d, bw): return torch.exp(-0.5*((d/bw))**2) / (bw*math.sqrt(2*math.pi))\n\n\ndef plot_func(f):\n    x = torch.linspace(0,10,100)\n    plt.plot(x, f(x))\n\n\nplot_func(partial(gaussian, bw=2.5))\n\n\n\n\n:::{{ callout-note }} ## Partial functions are cool :::\n\npartial\n\nfunctools.partial\n\n\nIn our implementation, we choose the bandwidth to be 2.5.\nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n\ndef tri(d, i): return (-d+i).clamp_min(0)/i\n\n\nplot_func(partial(tri, i=8))\n\n\n\n\n\nX = data.clone()\nx = data[0]\n\n\nx\n\ntensor([26.204, 26.349])\n\n\n\nx.shape,X.shape,x[None].shape\n\n(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))\n\n\n\n(x[None]-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n(x-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n# rewrite using torch.einsum\ndist = ((x-X)**2).sum(1).sqrt()\ndist[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])\n\n\n\nweight = gaussian(dist, 2.5)\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([1500]), torch.Size([1500, 2]))\n\n\n\nweight[:,None].shape\n\ntorch.Size([1500, 1])\n\n\n\nweight[:,None]*X\n\ntensor([[    4.182,     4.205],\n        [    1.215,     1.429],\n        [    0.749,     0.706],\n        ...,\n        [    0.000,     0.000],\n        [    0.000,     0.000],\n        [    0.000,     0.000]])\n\n\n\ndef one_update(X):\n    for i, x in enumerate(X):\n        dist = torch.sqrt(((x-X)**2).sum(1))\n#         weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        X[i] = (weight[:,None]*X).sum(0)/weight.sum()\n\n\ndef meanshift(data):\n    X = data.clone()\n    for it in range(5): one_update(X)\n    return X\n\n\n%time X=meanshift(data)\n\nCPU times: user 453 ms, sys: 0 ns, total: 453 ms\nWall time: 452 ms\n\n\n\nplot_data(centroids+2, X, n_samples)"
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html#animation",
    "href": "posts/course22p2/02_meanshift.html#animation",
    "title": "02 - Mean Shift",
    "section": "Animation",
    "text": "Animation\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\ndef do_one(d):\n    if d: one_update(X)\n    ax.clear()\n    plot_data(centroids+2, X, n_samples, ax=ax)\n\n\n# create your own animation\nX = data.clone()\nfig,ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames=5, interval=500, repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/course22p2/02_meanshift.html#gpu-batched-algorithm",
    "href": "posts/course22p2/02_meanshift.html#gpu-batched-algorithm",
    "title": "02 - Mean Shift",
    "section": "GPU batched algorithm",
    "text": "GPU batched algorithm\nTo truly accelerate the algorithm, we need to be performing updates on a batch of points per iteration, instead of just one as we were doing.\n\nbs=5\nX = data.clone()\nx = X[:bs]\nx.shape,X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a,b): return (((a[None]-b[:,None])**2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(X, x).shape\n\ntorch.Size([5, 1500])\n\n\n\nX[None,:].shape, x[:,None].shape, (X[None,:]-x[:,None]).shape\n\n(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))\n\n\n\nweight = gaussian(dist_b(X, x), 2)\nweight\n\ntensor([[    0.199,     0.030,     0.011,  ...,     0.000,     0.000,     0.000],\n        [    0.030,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.011,     0.009,     0.199,  ...,     0.000,     0.000,     0.000],\n        [    0.035,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.001,     0.004,     0.144,  ...,     0.000,     0.000,     0.000]])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([5, 1500]), torch.Size([1500, 2]))\n\n\n\nweight[...,None].shape, X[None].shape\n\n(torch.Size([5, 1500, 1]), torch.Size([1, 1500, 2]))\n\n\n\nnum = (weight[...,None]*X[None]).sum(1)\nnum.shape\n\ntorch.Size([5, 2])\n\n\n\nnum\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\ntorch.einsum('ij,jk->ik', weight, X)\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\nweight@X\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\ndiv = weight.sum(1, keepdim=True)\ndiv.shape\n\ntorch.Size([5, 1])\n\n\n\nnum/div\n\ntensor([[26.376, 27.692],\n        [26.101, 29.643],\n        [28.892, 28.990],\n        [26.071, 29.559],\n        [29.323, 29.685]])\n\n\n\ndef meanshift(data, bs=500):\n    n = len(data)\n    X = data.clone()\n    for it in range(5):\n        for i in range(0, n, bs):\n            s = slice(i, min(i+bs,n))\n            weight = gaussian(dist_b(X, X[s]), 2.5)\n#             weight = tri(dist_b(X, X[s]), 8)\n            div = weight.sum(1, keepdim=True)\n            X[s] = weight@X/div\n    return X\n\nAlthough each iteration still has to launch a new cuda kernel, there are now fewer iterations, and the acceleration from updating a batch of points more than makes up for it.\n\ndata = data.cuda()\n\n\nX = meanshift(data).cpu()\n\n\n%timeit -n 5 _=meanshift(data, 1250).cpu()\n\n2 ms ± 226 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nplot_data(centroids+2, X, n_samples)\n\n\n\n\nHomework: implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version.\nBonus: Implement it in APL too!\nSuper bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time.\nSuper super bonus: Publish a paper that describes it :D"
  },
  {
    "objectID": "posts/myfirstNN/2022-01-15-myFirstNN.html",
    "href": "posts/myfirstNN/2022-01-15-myFirstNN.html",
    "title": "Tiny bug world with neuro evolution (2018)",
    "section": "",
    "text": "This is from 2018, tutorials and trainings aside my first attempt for training a neural network. It does not have a traditional loss function or optimization method but it uses neuroevolution for training. I use p5.JS for the visualization and tensorflow.JS for the model.\nGithub Link is here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]